{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DSC 275/475: Time Series Analysis and Forecasting (Fall 2019)\n",
    "## Project 2.2.2 â€“ Sequence Classification with Recurrent Neural Networks\n",
    "### Chunlei Zhou"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import unicode_literals, print_function, division\n",
    "from io import open\n",
    "import glob\n",
    "import os\n",
    "import unicodedata\n",
    "import string\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import random\n",
    "import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from torch import LongTensor\n",
    "from torch.nn import Embedding\n",
    "from sklearn.metrics import accuracy_score\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN_Batch(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(RNN_Batch, self).__init__()\n",
    "        self.rnn = nn.RNN(\n",
    "            input_size=n_letters,\n",
    "            hidden_size=128, \n",
    "            num_layers=1,  \n",
    "            batch_first=True,\n",
    "        )\n",
    "        self.out = nn.Linear(128, n_categories)\n",
    "\n",
    "    def forward(self, x):\n",
    "        r_out, h = self.rnn(x, None)\n",
    "        out = self.out(r_out[:,-1,:])\n",
    "        return out\n",
    "\n",
    "class LSTM(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LSTM, self).__init__()\n",
    "\n",
    "        self.rnn = nn.LSTM(\n",
    "            input_size = n_letters,\n",
    "            hidden_size = 128,\n",
    "            num_layers = 1,\n",
    "            batch_first = True,\n",
    "        )\n",
    "        self.out = nn.Linear(128, n_categories)\n",
    "\n",
    "    def forward(self, x):\n",
    "        r_out, (h_n, h_c) = self.rnn(x, None)   \n",
    "        out = self.out(r_out[:, -1, :])\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def findFiles(path):\n",
    "    return glob.glob(path)\n",
    "\n",
    "all_letters = string.ascii_letters + \" .,;'\"\n",
    "n_letters = len(all_letters)\n",
    "\n",
    "# Turn a Unicode string to plain ASCII, thanks to https://stackoverflow.com/a/518232/2809427\n",
    "def unicodeToAscii(s):\n",
    "    return ''.join(\n",
    "        c for c in unicodedata.normalize('NFD', s)\n",
    "        if unicodedata.category(c) != 'Mn'\n",
    "        and c in all_letters)\n",
    "\n",
    "# Build the category_lines dictionary, a list of names per language\n",
    "category_lines = {}\n",
    "all_categories = []\n",
    "\n",
    "# Read a file and split into lines\n",
    "def readLines(filename):\n",
    "    lines = open(filename, encoding='utf-8').read().strip().split('\\n')\n",
    "    return [unicodeToAscii(line) for line in lines]\n",
    "\n",
    "for filename in findFiles('data/names/*.txt'):\n",
    "    category = os.path.splitext(os.path.basename(filename))[0]\n",
    "    all_categories.append(category)\n",
    "    lines = readLines(filename)\n",
    "    category_lines[category] = lines\n",
    "\n",
    "n_categories = len(all_categories)\n",
    "\n",
    "# Construct a Data Frame\n",
    "def data_frame(category_lines):\n",
    "    data_frame = [[],[]]\n",
    "    for key in all_categories:\n",
    "        for value in category_lines[key]:\n",
    "            data_frame[0].append(value)\n",
    "            data_frame[1].append(key)\n",
    "    return data_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "DF = data_frame(category_lines)\n",
    "total_samples = len(DF[0])\n",
    "randomize_order = np.arange(0, total_samples)\n",
    "learning_rate = 0.005"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 Batch Training of Data\n",
    "## 1.1 Leverage the RNN for batch training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Abl', 'Adsit', 'Ajdrna', 'Alt', 'Antonowitsch']\n",
      "[[5, 32, 42], [5, 34, 49, 39, 50], [5, 40, 34, 48, 44, 31], [5, 42, 50], [5, 44, 50, 45, 44, 45, 53, 39, 50, 49, 33, 38]]\n",
      "tensor([[ 5, 32, 42,  ...,  0,  0,  0],\n",
      "        [ 5, 34, 49,  ...,  0,  0,  0],\n",
      "        [ 5, 40, 34,  ...,  0,  0,  0],\n",
      "        ...,\n",
      "        [30, 39, 35,  ...,  0,  0,  0],\n",
      "        [30, 39, 35,  ...,  0,  0,  0],\n",
      "        [30, 51, 48,  ...,  0,  0,  0]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([20074, 19])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Padding\n",
    "feature = sorted(set(all_letters))\n",
    "vectorized_seqs = [[feature.index(tok) for tok in seq]for seq in DF[0]]\n",
    "print(DF[0][:5])\n",
    "print(vectorized_seqs[:5])\n",
    "embed = Embedding(len(feature), n_letters)\n",
    "seq_lengths = LongTensor(list(map(len, vectorized_seqs)))\n",
    "seq_tensor = Variable(torch.zeros((len(vectorized_seqs), seq_lengths.max()))).long()\n",
    "for idx, (seq, seqlen) in enumerate(zip(vectorized_seqs, seq_lengths)):\n",
    "    seq_tensor[idx, :seqlen] = LongTensor(seq)\n",
    "print(seq_tensor)\n",
    "seq_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 1.5352,  0.5698, -0.3237,  ..., -2.4026,  1.1965,  0.7177],\n",
      "         [-0.4861,  0.0046, -0.2793,  ..., -0.1003, -2.1389,  0.4859],\n",
      "         [ 1.2304,  0.7228,  1.5585,  ...,  1.1644,  1.1901,  1.9890],\n",
      "         ...,\n",
      "         [-0.1933, -0.7656,  1.7654,  ...,  0.5199,  1.9145, -0.4654],\n",
      "         [-0.1933, -0.7656,  1.7654,  ...,  0.5199,  1.9145, -0.4654],\n",
      "         [-0.1933, -0.7656,  1.7654,  ...,  0.5199,  1.9145, -0.4654]],\n",
      "\n",
      "        [[ 1.5352,  0.5698, -0.3237,  ..., -2.4026,  1.1965,  0.7177],\n",
      "         [-1.3778,  0.2630,  1.3235,  ...,  3.4291,  0.5275,  0.1177],\n",
      "         [-1.4819, -0.6776, -0.5344,  ..., -0.7741, -0.0990,  1.1569],\n",
      "         ...,\n",
      "         [-0.1933, -0.7656,  1.7654,  ...,  0.5199,  1.9145, -0.4654],\n",
      "         [-0.1933, -0.7656,  1.7654,  ...,  0.5199,  1.9145, -0.4654],\n",
      "         [-0.1933, -0.7656,  1.7654,  ...,  0.5199,  1.9145, -0.4654]],\n",
      "\n",
      "        [[ 1.5352,  0.5698, -0.3237,  ..., -2.4026,  1.1965,  0.7177],\n",
      "         [-0.3903, -0.7238,  0.8776,  ...,  2.6898, -0.5310,  1.0398],\n",
      "         [-1.3778,  0.2630,  1.3235,  ...,  3.4291,  0.5275,  0.1177],\n",
      "         ...,\n",
      "         [-0.1933, -0.7656,  1.7654,  ...,  0.5199,  1.9145, -0.4654],\n",
      "         [-0.1933, -0.7656,  1.7654,  ...,  0.5199,  1.9145, -0.4654],\n",
      "         [-0.1933, -0.7656,  1.7654,  ...,  0.5199,  1.9145, -0.4654]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 0.8806, -1.5975, -1.0972,  ..., -1.0782,  1.5165,  0.0103],\n",
      "         [-1.7357,  1.5281,  0.5959,  ...,  0.7232, -0.2474,  0.6927],\n",
      "         [-1.0448,  0.2545, -0.9945,  ...,  1.2080, -1.2375,  0.8536],\n",
      "         ...,\n",
      "         [-0.1933, -0.7656,  1.7654,  ...,  0.5199,  1.9145, -0.4654],\n",
      "         [-0.1933, -0.7656,  1.7654,  ...,  0.5199,  1.9145, -0.4654],\n",
      "         [-0.1933, -0.7656,  1.7654,  ...,  0.5199,  1.9145, -0.4654]],\n",
      "\n",
      "        [[ 0.8806, -1.5975, -1.0972,  ..., -1.0782,  1.5165,  0.0103],\n",
      "         [-1.7357,  1.5281,  0.5959,  ...,  0.7232, -0.2474,  0.6927],\n",
      "         [-1.0448,  0.2545, -0.9945,  ...,  1.2080, -1.2375,  0.8536],\n",
      "         ...,\n",
      "         [-0.1933, -0.7656,  1.7654,  ...,  0.5199,  1.9145, -0.4654],\n",
      "         [-0.1933, -0.7656,  1.7654,  ...,  0.5199,  1.9145, -0.4654],\n",
      "         [-0.1933, -0.7656,  1.7654,  ...,  0.5199,  1.9145, -0.4654]],\n",
      "\n",
      "        [[ 0.8806, -1.5975, -1.0972,  ..., -1.0782,  1.5165,  0.0103],\n",
      "         [ 0.2261,  1.3384, -1.4610,  ...,  0.2198, -0.1417, -0.2855],\n",
      "         [-0.2498, -1.2529, -0.1688,  ..., -0.7255, -0.8254,  0.2781],\n",
      "         ...,\n",
      "         [-0.1933, -0.7656,  1.7654,  ...,  0.5199,  1.9145, -0.4654],\n",
      "         [-0.1933, -0.7656,  1.7654,  ...,  0.5199,  1.9145, -0.4654],\n",
      "         [-0.1933, -0.7656,  1.7654,  ...,  0.5199,  1.9145, -0.4654]]],\n",
      "       grad_fn=<EmbeddingBackward>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([20074, 19, 57])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedded_seq_tensor = embed(seq_tensor)\n",
    "print(embedded_seq_tensor)\n",
    "embedded_seq_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Czech', 'Czech', 'Czech', 'Czech', 'Czech']\n",
      "[2, 2, 2, 2, 2]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([20074])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target = sorted(set(all_categories))\n",
    "vectorized_y = [target.index(tok) for tok in DF[1]]\n",
    "print(DF[1][:5])\n",
    "print(vectorized_y[:5])\n",
    "target_tensor = torch.tensor(vectorized_y,dtype = torch.long)\n",
    "target_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======= 1.1 Accuracy Report =======\n",
      "Batch Size: 20074\n"
     ]
    }
   ],
   "source": [
    "n_hidden = 128\n",
    "batch_size = total_samples\n",
    "print('======= 1.1 Accuracy Report =======')\n",
    "print('Batch Size:', batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epoch = 5\n",
    "rnn = RNN_Batch()\n",
    "optimizer = torch.optim.Adam(rnn.parameters(), lr=learning_rate)\n",
    "loss_func = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = [embedded_seq_tensor,target_tensor]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  0 | train loss: 2.8147 | test accuracy: 0.036067\n",
      "Epoch:  1 | train loss: 2.0949 | test accuracy: 0.468666\n",
      "Epoch:  2 | train loss: 1.9233 | test accuracy: 0.468666\n",
      "Epoch:  3 | train loss: 1.8931 | test accuracy: 0.468666\n",
      "Epoch:  4 | train loss: 1.8866 | test accuracy: 0.468666\n"
     ]
    }
   ],
   "source": [
    "x = batch[0]\n",
    "y = batch[1]\n",
    "n_epoch = 5\n",
    "for epoch in range(n_epoch): \n",
    "    optimizer.zero_grad()\n",
    "    output = rnn(x)\n",
    "    loss = loss_func(output, y) \n",
    "    loss.backward(retain_graph=True)\n",
    "    optimizer.step()\n",
    "    pred = torch.max(output, 1)[1]\n",
    "    accuracy = accuracy_score(y, pred)\n",
    "    print(\"Epoch: \", epoch, \"| train loss: %.4f\" % loss.item(), '| test accuracy: %f' % accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Arbitrary mini-batch sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======= 1.2 Accuracy Report =======\n",
      "Batch Size = 1000\n",
      "Epoch:  0 | train loss: 1.8513 | test accuracy: 0.469400\n",
      "Epoch:  1 | train loss: 1.8439 | test accuracy: 0.468950\n",
      "Epoch:  2 | train loss: 1.8430 | test accuracy: 0.469100\n",
      "Epoch:  3 | train loss: 1.8445 | test accuracy: 0.468350\n",
      "Epoch:  4 | train loss: 1.8435 | test accuracy: 0.468750\n",
      "Epoch:  5 | train loss: 1.8449 | test accuracy: 0.468500\n",
      "Epoch:  6 | train loss: 1.8444 | test accuracy: 0.468650\n",
      "Epoch:  7 | train loss: 1.8445 | test accuracy: 0.468450\n",
      "Epoch:  8 | train loss: 1.8439 | test accuracy: 0.468800\n",
      "Epoch:  9 | train loss: 1.8433 | test accuracy: 0.468650\n",
      "Epoch:  10 | train loss: 1.8442 | test accuracy: 0.468400\n",
      "Epoch:  11 | train loss: 1.8442 | test accuracy: 0.468500\n",
      "Epoch:  12 | train loss: 1.8430 | test accuracy: 0.468750\n",
      "Epoch:  13 | train loss: 1.8412 | test accuracy: 0.468750\n",
      "Epoch:  14 | train loss: 1.8480 | test accuracy: 0.468700\n",
      "Epoch:  15 | train loss: 1.8450 | test accuracy: 0.468550\n",
      "Epoch:  16 | train loss: 1.8440 | test accuracy: 0.468750\n",
      "Epoch:  17 | train loss: 1.8446 | test accuracy: 0.468400\n",
      "Epoch:  18 | train loss: 1.8436 | test accuracy: 0.468950\n",
      "Epoch:  19 | train loss: 1.8445 | test accuracy: 0.468700\n",
      "Batch Size = 2000\n",
      "Epoch:  0 | train loss: 1.8697 | test accuracy: 0.468950\n",
      "Epoch:  1 | train loss: 1.8485 | test accuracy: 0.468750\n",
      "Epoch:  2 | train loss: 1.8465 | test accuracy: 0.469050\n",
      "Epoch:  3 | train loss: 1.8448 | test accuracy: 0.468900\n",
      "Epoch:  4 | train loss: 1.8457 | test accuracy: 0.468400\n",
      "Epoch:  5 | train loss: 1.8437 | test accuracy: 0.468850\n",
      "Epoch:  6 | train loss: 1.8427 | test accuracy: 0.469000\n",
      "Epoch:  7 | train loss: 1.8449 | test accuracy: 0.468450\n",
      "Epoch:  8 | train loss: 1.8450 | test accuracy: 0.468900\n",
      "Epoch:  9 | train loss: 1.8457 | test accuracy: 0.468450\n",
      "Batch Size = 5000\n",
      "Epoch:  0 | train loss: 1.8801 | test accuracy: 0.468400\n",
      "Epoch:  1 | train loss: 1.8725 | test accuracy: 0.468500\n",
      "Epoch:  2 | train loss: 1.8571 | test accuracy: 0.468700\n",
      "Epoch:  3 | train loss: 1.8506 | test accuracy: 0.468800\n"
     ]
    }
   ],
   "source": [
    "print('======= 1.2 Accuracy Report =======')\n",
    "batch_sizes = [1000, 2000, 5000]\n",
    "N = 20000\n",
    "for batch_size in batch_sizes:\n",
    "    print('Batch Size =', batch_size)\n",
    "    rnn = RNN_Batch()\n",
    "    optimizer = torch.optim.Adam(rnn.parameters(), lr=learning_rate)\n",
    "    loss_func = nn.CrossEntropyLoss()\n",
    "    n_epoch = int(N/batch_size)\n",
    "    for epoch in range(n_epoch):\n",
    "        samples = random.sample(list(randomize_order), 20000)\n",
    "        sample_index = [samples[i * batch_size:(i + 1) * batch_size] for i in range((len(samples) + batch_size - 1) // batch_size )]\n",
    "        batch_tensors = []\n",
    "        target_tensors = []\n",
    "        sample = []\n",
    "        target_output = []\n",
    "        for index in sample_index:\n",
    "            for i in index:\n",
    "                sample.append(DF[0][i])\n",
    "                target_output.append(DF[1][i])\n",
    "            vectorized_seqs = [[feature.index(tok) for tok in seq]for seq in sample]\n",
    "            seq_lengths = LongTensor(list(map(len, vectorized_seqs)))\n",
    "            seq_tensor = Variable(torch.zeros((len(vectorized_seqs), seq_lengths.max()))).long()\n",
    "            for idx, (seq, seqlen) in enumerate(zip(vectorized_seqs, seq_lengths)):\n",
    "                seq_tensor[idx, :seqlen] = LongTensor(seq)\n",
    "            embedded_seq_tensor = embed(seq_tensor)\n",
    "            batch_tensors.append(embedded_seq_tensor)\n",
    "            vectorized_y = [target.index(tok) for tok in target_output]\n",
    "            target_tensor = torch.tensor(vectorized_y,dtype = torch.long)\n",
    "            target_tensors.append(target_tensor)  \n",
    "        for j in range(n_epoch):\n",
    "            batch = [batch_tensors[j],target_tensors[j]]\n",
    "            x = batch[0]\n",
    "            y = batch[1]\n",
    "            optimizer.zero_grad()\n",
    "            output = rnn(x)\n",
    "            loss = loss_func(output, y) \n",
    "            loss.backward(retain_graph=True)\n",
    "            optimizer.step()\n",
    "        test_output = rnn(embedded_seq_tensor)\n",
    "        pred = torch.max(test_output, 1)[1]\n",
    "        accuracy = accuracy_score(y, pred)\n",
    "        print(\"Epoch: \", epoch, \"| train loss: %.4f\" % loss.item(), '| test accuracy: %f' % accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Model cross-validation\n",
    "## 2.1 Five-Fold Cross-Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Abl', 'Adsit', 'Ajdrna', 'Alt', 'Antonowitsch']\n",
      "[[5, 32, 42], [5, 34, 49, 39, 50], [5, 40, 34, 48, 44, 31], [5, 42, 50], [5, 44, 50, 45, 44, 45, 53, 39, 50, 49, 33, 38]]\n",
      "tensor([[ 5, 32, 42,  ...,  0,  0,  0],\n",
      "        [ 5, 34, 49,  ...,  0,  0,  0],\n",
      "        [ 5, 40, 34,  ...,  0,  0,  0],\n",
      "        ...,\n",
      "        [30, 39, 35,  ...,  0,  0,  0],\n",
      "        [30, 39, 35,  ...,  0,  0,  0],\n",
      "        [30, 51, 48,  ...,  0,  0,  0]])\n",
      "tensor([[[-0.7040, -0.2269,  1.2852,  ...,  1.3787, -0.7968,  1.3350],\n",
      "         [ 1.0751,  1.3456,  0.2650,  ...,  0.0063,  0.3570,  0.9540],\n",
      "         [ 0.8692,  0.9518, -2.3914,  ..., -0.9330,  0.1102, -0.8087],\n",
      "         ...,\n",
      "         [-0.1318,  0.7543,  0.5666,  ...,  0.7242, -1.1255, -0.8621],\n",
      "         [-0.1318,  0.7543,  0.5666,  ...,  0.7242, -1.1255, -0.8621],\n",
      "         [-0.1318,  0.7543,  0.5666,  ...,  0.7242, -1.1255, -0.8621]],\n",
      "\n",
      "        [[-0.7040, -0.2269,  1.2852,  ...,  1.3787, -0.7968,  1.3350],\n",
      "         [ 2.3416, -0.7289,  0.7296,  ..., -0.3200, -0.3559,  0.6245],\n",
      "         [ 1.5489, -0.6545, -1.5817,  ..., -0.3902,  0.6307,  1.5962],\n",
      "         ...,\n",
      "         [-0.1318,  0.7543,  0.5666,  ...,  0.7242, -1.1255, -0.8621],\n",
      "         [-0.1318,  0.7543,  0.5666,  ...,  0.7242, -1.1255, -0.8621],\n",
      "         [-0.1318,  0.7543,  0.5666,  ...,  0.7242, -1.1255, -0.8621]],\n",
      "\n",
      "        [[-0.7040, -0.2269,  1.2852,  ...,  1.3787, -0.7968,  1.3350],\n",
      "         [ 1.0011,  0.0927, -0.5699,  ...,  0.1118, -0.7145, -0.4338],\n",
      "         [ 2.3416, -0.7289,  0.7296,  ..., -0.3200, -0.3559,  0.6245],\n",
      "         ...,\n",
      "         [-0.1318,  0.7543,  0.5666,  ...,  0.7242, -1.1255, -0.8621],\n",
      "         [-0.1318,  0.7543,  0.5666,  ...,  0.7242, -1.1255, -0.8621],\n",
      "         [-0.1318,  0.7543,  0.5666,  ...,  0.7242, -1.1255, -0.8621]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-1.1003,  0.2088, -1.1226,  ..., -0.7057,  0.2012, -0.2845],\n",
      "         [-0.5551,  0.2768, -2.0776,  ..., -0.7528, -1.1124,  0.8219],\n",
      "         [-0.3918, -0.5478,  0.6534,  ...,  1.7709, -0.5518,  0.0627],\n",
      "         ...,\n",
      "         [-0.1318,  0.7543,  0.5666,  ...,  0.7242, -1.1255, -0.8621],\n",
      "         [-0.1318,  0.7543,  0.5666,  ...,  0.7242, -1.1255, -0.8621],\n",
      "         [-0.1318,  0.7543,  0.5666,  ...,  0.7242, -1.1255, -0.8621]],\n",
      "\n",
      "        [[-1.1003,  0.2088, -1.1226,  ..., -0.7057,  0.2012, -0.2845],\n",
      "         [-0.5551,  0.2768, -2.0776,  ..., -0.7528, -1.1124,  0.8219],\n",
      "         [-0.3918, -0.5478,  0.6534,  ...,  1.7709, -0.5518,  0.0627],\n",
      "         ...,\n",
      "         [-0.1318,  0.7543,  0.5666,  ...,  0.7242, -1.1255, -0.8621],\n",
      "         [-0.1318,  0.7543,  0.5666,  ...,  0.7242, -1.1255, -0.8621],\n",
      "         [-0.1318,  0.7543,  0.5666,  ...,  0.7242, -1.1255, -0.8621]],\n",
      "\n",
      "        [[-1.1003,  0.2088, -1.1226,  ..., -0.7057,  0.2012, -0.2845],\n",
      "         [-0.1983,  0.0035, -0.4496,  ..., -0.8481,  2.2269, -0.3073],\n",
      "         [ 0.4732,  1.1128, -0.6893,  ..., -0.8446,  0.5728, -0.6708],\n",
      "         ...,\n",
      "         [-0.1318,  0.7543,  0.5666,  ...,  0.7242, -1.1255, -0.8621],\n",
      "         [-0.1318,  0.7543,  0.5666,  ...,  0.7242, -1.1255, -0.8621],\n",
      "         [-0.1318,  0.7543,  0.5666,  ...,  0.7242, -1.1255, -0.8621]]],\n",
      "       grad_fn=<EmbeddingBackward>)\n",
      "torch.Size([20074, 19, 57])\n",
      "['Czech', 'Czech', 'Czech', 'Czech', 'Czech']\n",
      "[2, 2, 2, 2, 2]\n",
      "torch.Size([20074])\n"
     ]
    }
   ],
   "source": [
    "feature = sorted(set(all_letters))\n",
    "vectorized_seqs = [[feature.index(tok) for tok in seq]for seq in DF[0]]\n",
    "print(DF[0][:5])\n",
    "print(vectorized_seqs[:5])\n",
    "embed = Embedding(len(feature), n_letters)\n",
    "seq_lengths = LongTensor(list(map(len, vectorized_seqs)))\n",
    "seq_tensor = Variable(torch.zeros((len(vectorized_seqs), seq_lengths.max()))).long()\n",
    "for idx, (seq, seqlen) in enumerate(zip(vectorized_seqs, seq_lengths)):\n",
    "    seq_tensor[idx, :seqlen] = LongTensor(seq)\n",
    "print(seq_tensor)\n",
    "embedded_seq_tensor = embed(seq_tensor)\n",
    "print(embedded_seq_tensor)\n",
    "print(embedded_seq_tensor.shape)\n",
    "target = sorted(set(all_categories))\n",
    "vectorized_y = [target.index(tok) for tok in DF[1]]\n",
    "print(DF[1][:5])\n",
    "print(vectorized_y[:5])\n",
    "target_tensor = torch.tensor(vectorized_y,dtype = torch.long)\n",
    "print(target_tensor.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  (a) Accuracy Report After Five Epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('======= 2.1(a) Accuracy Report After Five Epochs =======')\n",
    "kfold_cv = KFold(n_splits=5, random_state=None, shuffle=True)\n",
    "rnn = RNN_Batch()\n",
    "optimizer = torch.optim.Adam(rnn.parameters(), lr=learning_rate)\n",
    "loss_func = nn.CrossEntropyLoss()\n",
    "n_epoch = 5 \n",
    "for epoch in range(n_epoch):\n",
    "    accuracies_test = []\n",
    "    accuracies_train = []\n",
    "    for train_index, test_index in kfold_cv.split(np.arange(0, total_samples)):\n",
    "        X_train, X_test, y_train, y_test = embedded_seq_tensor[train_index], embedded_seq_tensor[test_index], target_tensor[train_index], target_tensor[test_index]\n",
    "        optimizer.zero_grad()\n",
    "        output = rnn(X_train)\n",
    "        loss = loss_func(output, y_train) \n",
    "        loss.backward(retain_graph=True)\n",
    "        optimizer.step()\n",
    "        test_output = rnn(X_test)\n",
    "        train_output = rnn(X_train)\n",
    "        pred_test = torch.max(test_output, 1)[1]\n",
    "        pred_train = torch.max(train_output, 1)[1]\n",
    "        accuracies_test.append(accuracy_score(y_test, pred_test))\n",
    "        accuracies_train.append(accuracy_score(y_train, pred_train))\n",
    "    print(\"Epoch: \", epoch, \"| Train Loss: %.4f\" % loss.item(), '| Test accuracy: %f' % np.mean(accuracies_test))\n",
    "    print(\"Epoch: \", epoch, \"| Train Loss: %.4f\" % loss.item(), '| Train accuracy: %f' % np.mean(accuracies_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (b) Accuracy Report After Chosen Epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Abl', 'Adsit', 'Ajdrna', 'Alt', 'Antonowitsch']\n",
      "[[5, 32, 42], [5, 34, 49, 39, 50], [5, 40, 34, 48, 44, 31], [5, 42, 50], [5, 44, 50, 45, 44, 45, 53, 39, 50, 49, 33, 38]]\n",
      "tensor([3, 5, 6,  ..., 8, 7, 5])\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]])\n",
      "tensor([[ 5, 32, 42,  ...,  0,  0,  0],\n",
      "        [ 5, 34, 49,  ...,  0,  0,  0],\n",
      "        [ 5, 40, 34,  ...,  0,  0,  0],\n",
      "        ...,\n",
      "        [30, 39, 35,  ..., 31, 41,  0],\n",
      "        [30, 39, 35,  ..., 41,  0,  0],\n",
      "        [30, 51, 48,  ...,  0,  0,  0]])\n",
      "tensor([[[-0.1936, -1.2994, -0.4664,  ..., -0.3159, -1.3663, -0.5370],\n",
      "         [-0.4547,  0.2550,  1.6509,  ..., -0.7490, -1.1880,  1.0071],\n",
      "         [-0.3689, -2.0978,  0.3502,  ..., -0.6274,  0.1800, -0.4970],\n",
      "         ...,\n",
      "         [-0.6283,  1.4404,  1.8816,  ...,  0.0653, -0.3580, -0.2610],\n",
      "         [-0.6283,  1.4404,  1.8816,  ...,  0.0653, -0.3580, -0.2610],\n",
      "         [-0.6283,  1.4404,  1.8816,  ...,  0.0653, -0.3580, -0.2610]],\n",
      "\n",
      "        [[-0.1936, -1.2994, -0.4664,  ..., -0.3159, -1.3663, -0.5370],\n",
      "         [ 0.2666, -0.1544, -0.3361,  ..., -2.5416, -0.6602, -0.0660],\n",
      "         [-1.0168, -0.3097, -0.2526,  ...,  0.5633, -1.2930,  1.5371],\n",
      "         ...,\n",
      "         [-0.6283,  1.4404,  1.8816,  ...,  0.0653, -0.3580, -0.2610],\n",
      "         [-0.6283,  1.4404,  1.8816,  ...,  0.0653, -0.3580, -0.2610],\n",
      "         [-0.6283,  1.4404,  1.8816,  ...,  0.0653, -0.3580, -0.2610]],\n",
      "\n",
      "        [[-0.1936, -1.2994, -0.4664,  ..., -0.3159, -1.3663, -0.5370],\n",
      "         [ 1.7334, -1.6257, -0.5374,  ..., -0.1733,  0.9724, -0.5335],\n",
      "         [ 0.2666, -0.1544, -0.3361,  ..., -2.5416, -0.6602, -0.0660],\n",
      "         ...,\n",
      "         [-0.6283,  1.4404,  1.8816,  ...,  0.0653, -0.3580, -0.2610],\n",
      "         [-0.6283,  1.4404,  1.8816,  ...,  0.0653, -0.3580, -0.2610],\n",
      "         [-0.6283,  1.4404,  1.8816,  ...,  0.0653, -0.3580, -0.2610]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 0.8010, -0.5808,  0.5872,  ...,  0.7438, -1.6940,  1.6854],\n",
      "         [-1.8229,  0.3823, -1.6246,  ..., -2.5432, -2.1004, -0.3359],\n",
      "         [ 0.1050,  0.0952,  0.3282,  ...,  0.4262,  0.6018,  0.0958],\n",
      "         ...,\n",
      "         [-0.3639, -1.1791, -0.0749,  ...,  0.8762,  0.8909,  1.1228],\n",
      "         [ 2.0418, -0.7351, -0.1197,  ...,  0.5509, -0.8678, -0.0466],\n",
      "         [-0.6283,  1.4404,  1.8816,  ...,  0.0653, -0.3580, -0.2610]],\n",
      "\n",
      "        [[ 0.8010, -0.5808,  0.5872,  ...,  0.7438, -1.6940,  1.6854],\n",
      "         [-1.8229,  0.3823, -1.6246,  ..., -2.5432, -2.1004, -0.3359],\n",
      "         [ 0.1050,  0.0952,  0.3282,  ...,  0.4262,  0.6018,  0.0958],\n",
      "         ...,\n",
      "         [ 2.0418, -0.7351, -0.1197,  ...,  0.5509, -0.8678, -0.0466],\n",
      "         [-0.6283,  1.4404,  1.8816,  ...,  0.0653, -0.3580, -0.2610],\n",
      "         [-0.6283,  1.4404,  1.8816,  ...,  0.0653, -0.3580, -0.2610]],\n",
      "\n",
      "        [[ 0.8010, -0.5808,  0.5872,  ...,  0.7438, -1.6940,  1.6854],\n",
      "         [ 0.0920, -0.5163, -1.0010,  ..., -1.3034, -0.4949,  0.0329],\n",
      "         [ 0.0900, -0.4625, -0.3321,  ...,  0.1414,  1.3594, -0.9187],\n",
      "         ...,\n",
      "         [-0.6283,  1.4404,  1.8816,  ...,  0.0653, -0.3580, -0.2610],\n",
      "         [-0.6283,  1.4404,  1.8816,  ...,  0.0653, -0.3580, -0.2610],\n",
      "         [-0.6283,  1.4404,  1.8816,  ...,  0.0653, -0.3580, -0.2610]]],\n",
      "       grad_fn=<EmbeddingBackward>)\n",
      "torch.Size([20074, 9, 57])\n",
      "['Czech', 'Czech', 'Czech', 'Czech', 'Czech']\n",
      "[2, 2, 2, 2, 2]\n",
      "torch.Size([20074])\n"
     ]
    }
   ],
   "source": [
    "# Padding\n",
    "feature = sorted(set(all_letters))\n",
    "vectorized_seqs = [[feature.index(tok) for tok in seq]for seq in DF[0]]\n",
    "print(DF[0][:5])\n",
    "print(vectorized_seqs[:5])\n",
    "embed = Embedding(len(feature), n_letters)\n",
    "seq_lengths = LongTensor(list(map(len, vectorized_seqs)))\n",
    "print(seq_lengths)\n",
    "seq_tensor = Variable(torch.zeros((len(vectorized_seqs), 9))).long()\n",
    "print(seq_tensor)\n",
    "seq_tensor.shape\n",
    "for idx, (seq, seqlen) in enumerate(zip(vectorized_seqs, seq_lengths)):\n",
    "    if seqlen <= 9:\n",
    "        seq_tensor[idx, :seqlen] = LongTensor(seq)\n",
    "    else:\n",
    "        seq_tensor[idx, :seqlen] = LongTensor(seq[:9])\n",
    "print(seq_tensor)\n",
    "seq_tensor.shape\n",
    "embedded_seq_tensor = embed(seq_tensor)\n",
    "print(embedded_seq_tensor)\n",
    "print(embedded_seq_tensor.shape)\n",
    "target = sorted(set(all_categories))\n",
    "vectorized_y = [target.index(tok) for tok in DF[1]]\n",
    "print(DF[1][:5])\n",
    "print(vectorized_y[:5])\n",
    "target_tensor = torch.tensor(vectorized_y,dtype = torch.long)\n",
    "print(target_tensor.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======= 2.1(b) Accuracy Report After Chosen Epochs =======\n",
      "Epoch:  0 | Train Loss: 1.7909 | Test accuracy: 0.449289\n",
      "Epoch:  0 | Train Loss: 1.7909 | Train accuracy: 0.449412\n",
      "Epoch:  1 | Train Loss: 1.6684 | Test accuracy: 0.495866\n",
      "Epoch:  1 | Train Loss: 1.6684 | Train accuracy: 0.496625\n",
      "Epoch:  2 | Train Loss: 1.6069 | Test accuracy: 0.511160\n",
      "Epoch:  2 | Train Loss: 1.6069 | Train accuracy: 0.512653\n",
      "Epoch:  3 | Train Loss: 1.5177 | Test accuracy: 0.527447\n",
      "Epoch:  3 | Train Loss: 1.5177 | Train accuracy: 0.525780\n",
      "Epoch:  4 | Train Loss: 1.5052 | Test accuracy: 0.537412\n",
      "Epoch:  4 | Train Loss: 1.5052 | Train accuracy: 0.538495\n",
      "Epoch:  5 | Train Loss: 1.4708 | Test accuracy: 0.560527\n",
      "Epoch:  5 | Train Loss: 1.4708 | Train accuracy: 0.561298\n",
      "Epoch:  6 | Train Loss: 1.4137 | Test accuracy: 0.583592\n",
      "Epoch:  6 | Train Loss: 1.4137 | Train accuracy: 0.582818\n",
      "Epoch:  7 | Train Loss: 1.3426 | Test accuracy: 0.608449\n",
      "Epoch:  7 | Train Loss: 1.3426 | Train accuracy: 0.608685\n",
      "Epoch:  8 | Train Loss: 1.3024 | Test accuracy: 0.621551\n",
      "Epoch:  8 | Train Loss: 1.3024 | Train accuracy: 0.620828\n",
      "Epoch:  9 | Train Loss: 1.2523 | Test accuracy: 0.633556\n",
      "Epoch:  9 | Train Loss: 1.2523 | Train accuracy: 0.632572\n",
      "Epoch:  10 | Train Loss: 1.2231 | Test accuracy: 0.638836\n",
      "Epoch:  10 | Train Loss: 1.2231 | Train accuracy: 0.641028\n",
      "Epoch:  11 | Train Loss: 1.2264 | Test accuracy: 0.647604\n",
      "Epoch:  11 | Train Loss: 1.2264 | Train accuracy: 0.645898\n",
      "Epoch:  12 | Train Loss: 1.1553 | Test accuracy: 0.663445\n",
      "Epoch:  12 | Train Loss: 1.1553 | Train accuracy: 0.661851\n",
      "Epoch:  13 | Train Loss: 1.1252 | Test accuracy: 0.674306\n",
      "Epoch:  13 | Train Loss: 1.1252 | Train accuracy: 0.674318\n",
      "Epoch:  14 | Train Loss: 1.0853 | Test accuracy: 0.686261\n",
      "Epoch:  14 | Train Loss: 1.0853 | Train accuracy: 0.686485\n",
      "Epoch:  15 | Train Loss: 1.0543 | Test accuracy: 0.699811\n",
      "Epoch:  15 | Train Loss: 1.0543 | Train accuracy: 0.700035\n",
      "Epoch:  16 | Train Loss: 1.0195 | Test accuracy: 0.709724\n",
      "Epoch:  16 | Train Loss: 1.0195 | Train accuracy: 0.709301\n",
      "Epoch:  17 | Train Loss: 0.9770 | Test accuracy: 0.715503\n",
      "Epoch:  17 | Train Loss: 0.9770 | Train accuracy: 0.714643\n",
      "Epoch:  18 | Train Loss: 0.9674 | Test accuracy: 0.726462\n",
      "Epoch:  18 | Train Loss: 0.9674 | Train accuracy: 0.726138\n",
      "Epoch:  19 | Train Loss: 0.9133 | Test accuracy: 0.733685\n",
      "Epoch:  19 | Train Loss: 0.9133 | Train accuracy: 0.734445\n",
      "Epoch:  20 | Train Loss: 0.8735 | Test accuracy: 0.742752\n",
      "Epoch:  20 | Train Loss: 0.8735 | Train accuracy: 0.744495\n",
      "Epoch:  21 | Train Loss: 0.9050 | Test accuracy: 0.733037\n",
      "Epoch:  21 | Train Loss: 0.9050 | Train accuracy: 0.735441\n",
      "Epoch:  22 | Train Loss: 0.9862 | Test accuracy: 0.722428\n",
      "Epoch:  22 | Train Loss: 0.9862 | Train accuracy: 0.725042\n",
      "Epoch:  23 | Train Loss: 0.9130 | Test accuracy: 0.734931\n",
      "Epoch:  23 | Train Loss: 0.9130 | Train accuracy: 0.735553\n",
      "Epoch:  24 | Train Loss: 0.8222 | Test accuracy: 0.751370\n",
      "Epoch:  24 | Train Loss: 0.8222 | Train accuracy: 0.750535\n",
      "Epoch:  25 | Train Loss: 0.8044 | Test accuracy: 0.760238\n",
      "Epoch:  25 | Train Loss: 0.8044 | Train accuracy: 0.760137\n",
      "Epoch:  26 | Train Loss: 0.7770 | Test accuracy: 0.770648\n",
      "Epoch:  26 | Train Loss: 0.7770 | Train accuracy: 0.770188\n",
      "Epoch:  27 | Train Loss: 0.7473 | Test accuracy: 0.778818\n",
      "Epoch:  27 | Train Loss: 0.7473 | Train accuracy: 0.779379\n",
      "Epoch:  28 | Train Loss: 0.7293 | Test accuracy: 0.787287\n",
      "Epoch:  28 | Train Loss: 0.7293 | Train accuracy: 0.786764\n",
      "Epoch:  29 | Train Loss: 0.7103 | Test accuracy: 0.788035\n",
      "Epoch:  29 | Train Loss: 0.7103 | Train accuracy: 0.789043\n",
      "Epoch:  30 | Train Loss: 0.7033 | Test accuracy: 0.792717\n",
      "Epoch:  30 | Train Loss: 0.7033 | Train accuracy: 0.794199\n",
      "Epoch:  31 | Train Loss: 0.6755 | Test accuracy: 0.795357\n",
      "Epoch:  31 | Train Loss: 0.6755 | Train accuracy: 0.797350\n",
      "Epoch:  32 | Train Loss: 0.6642 | Test accuracy: 0.804723\n",
      "Epoch:  32 | Train Loss: 0.6642 | Train accuracy: 0.805843\n",
      "Epoch:  33 | Train Loss: 0.6478 | Test accuracy: 0.807363\n",
      "Epoch:  33 | Train Loss: 0.6478 | Train accuracy: 0.806802\n",
      "Epoch:  34 | Train Loss: 0.6335 | Test accuracy: 0.811846\n",
      "Epoch:  34 | Train Loss: 0.6335 | Train accuracy: 0.811522\n",
      "Epoch:  35 | Train Loss: 0.6206 | Test accuracy: 0.815134\n",
      "Epoch:  35 | Train Loss: 0.6206 | Train accuracy: 0.815869\n",
      "Epoch:  36 | Train Loss: 0.5976 | Test accuracy: 0.818771\n",
      "Epoch:  36 | Train Loss: 0.5976 | Train accuracy: 0.818883\n",
      "Epoch:  37 | Train Loss: 0.6495 | Test accuracy: 0.811796\n",
      "Epoch:  37 | Train Loss: 0.6495 | Train accuracy: 0.813963\n",
      "Epoch:  38 | Train Loss: 0.6656 | Test accuracy: 0.805720\n",
      "Epoch:  38 | Train Loss: 0.6656 | Train accuracy: 0.804536\n",
      "Epoch:  39 | Train Loss: 0.5887 | Test accuracy: 0.819219\n",
      "Epoch:  39 | Train Loss: 0.5887 | Train accuracy: 0.818198\n",
      "Epoch:  40 | Train Loss: 0.5702 | Test accuracy: 0.824549\n",
      "Epoch:  40 | Train Loss: 0.5702 | Train accuracy: 0.825682\n",
      "Epoch:  41 | Train Loss: 0.5607 | Test accuracy: 0.828634\n",
      "Epoch:  41 | Train Loss: 0.5607 | Train accuracy: 0.829742\n",
      "Epoch:  42 | Train Loss: 0.5363 | Test accuracy: 0.832370\n",
      "Epoch:  42 | Train Loss: 0.5363 | Train accuracy: 0.832756\n",
      "Epoch:  43 | Train Loss: 0.5279 | Test accuracy: 0.834662\n",
      "Epoch:  43 | Train Loss: 0.5279 | Train accuracy: 0.834587\n",
      "Epoch:  44 | Train Loss: 0.5273 | Test accuracy: 0.837202\n",
      "Epoch:  44 | Train Loss: 0.5273 | Train accuracy: 0.837115\n",
      "Epoch:  45 | Train Loss: 0.5207 | Test accuracy: 0.841486\n",
      "Epoch:  45 | Train Loss: 0.5207 | Train accuracy: 0.841237\n",
      "Epoch:  46 | Train Loss: 0.4938 | Test accuracy: 0.842084\n",
      "Epoch:  46 | Train Loss: 0.4938 | Train accuracy: 0.843653\n",
      "Epoch:  47 | Train Loss: 0.5257 | Test accuracy: 0.837501\n",
      "Epoch:  47 | Train Loss: 0.5257 | Train accuracy: 0.840117\n",
      "Epoch:  48 | Train Loss: 0.5368 | Test accuracy: 0.832171\n",
      "Epoch:  48 | Train Loss: 0.5368 | Train accuracy: 0.832084\n",
      "Epoch:  49 | Train Loss: 0.5625 | Test accuracy: 0.829431\n",
      "Epoch:  49 | Train Loss: 0.5625 | Train accuracy: 0.829630\n",
      "Epoch:  50 | Train Loss: 0.4982 | Test accuracy: 0.832769\n",
      "Epoch:  50 | Train Loss: 0.4982 | Train accuracy: 0.833267\n",
      "Epoch:  51 | Train Loss: 0.4879 | Test accuracy: 0.842981\n",
      "Epoch:  51 | Train Loss: 0.4879 | Train accuracy: 0.841785\n",
      "Epoch:  52 | Train Loss: 0.4769 | Test accuracy: 0.851599\n",
      "Epoch:  52 | Train Loss: 0.4769 | Train accuracy: 0.852932\n",
      "Epoch:  53 | Train Loss: 0.4590 | Test accuracy: 0.857428\n",
      "Epoch:  53 | Train Loss: 0.4590 | Train accuracy: 0.856481\n",
      "Epoch:  54 | Train Loss: 0.4418 | Test accuracy: 0.862061\n",
      "Epoch:  54 | Train Loss: 0.4418 | Train accuracy: 0.862671\n",
      "Epoch:  55 | Train Loss: 0.4282 | Test accuracy: 0.865049\n",
      "Epoch:  55 | Train Loss: 0.4282 | Train accuracy: 0.866195\n",
      "Epoch:  56 | Train Loss: 0.4185 | Test accuracy: 0.869882\n",
      "Epoch:  56 | Train Loss: 0.4185 | Train accuracy: 0.869744\n",
      "Epoch:  57 | Train Loss: 0.4115 | Test accuracy: 0.872871\n",
      "Epoch:  57 | Train Loss: 0.4115 | Train accuracy: 0.873643\n",
      "Epoch:  58 | Train Loss: 0.3982 | Test accuracy: 0.876058\n",
      "Epoch:  58 | Train Loss: 0.3982 | Train accuracy: 0.875772\n",
      "Epoch:  59 | Train Loss: 0.3871 | Test accuracy: 0.879297\n",
      "Epoch:  59 | Train Loss: 0.3871 | Train accuracy: 0.879882\n",
      "Epoch:  60 | Train Loss: 0.5184 | Test accuracy: 0.845419\n",
      "Epoch:  60 | Train Loss: 0.5184 | Train accuracy: 0.843754\n",
      "Epoch:  61 | Train Loss: 0.8382 | Test accuracy: 0.785544\n",
      "Epoch:  61 | Train Loss: 0.8382 | Train accuracy: 0.786440\n",
      "Epoch:  62 | Train Loss: 0.6021 | Test accuracy: 0.798596\n",
      "Epoch:  62 | Train Loss: 0.6021 | Train accuracy: 0.800849\n",
      "Epoch:  63 | Train Loss: 0.5245 | Test accuracy: 0.825546\n",
      "Epoch:  63 | Train Loss: 0.5245 | Train accuracy: 0.827052\n",
      "Epoch:  64 | Train Loss: 0.4818 | Test accuracy: 0.842483\n",
      "Epoch:  64 | Train Loss: 0.4818 | Train accuracy: 0.843143\n",
      "Epoch:  65 | Train Loss: 0.4505 | Test accuracy: 0.853941\n",
      "Epoch:  65 | Train Loss: 0.4505 | Train accuracy: 0.855447\n",
      "Epoch:  66 | Train Loss: 0.4225 | Test accuracy: 0.867541\n",
      "Epoch:  66 | Train Loss: 0.4225 | Train accuracy: 0.868013\n",
      "Epoch:  67 | Train Loss: 0.3971 | Test accuracy: 0.874066\n",
      "Epoch:  67 | Train Loss: 0.3971 | Train accuracy: 0.874253\n",
      "Epoch:  68 | Train Loss: 0.3779 | Test accuracy: 0.880343\n",
      "Epoch:  68 | Train Loss: 0.3779 | Train accuracy: 0.880741\n",
      "Epoch:  69 | Train Loss: 0.3654 | Test accuracy: 0.885524\n",
      "Epoch:  69 | Train Loss: 0.3654 | Train accuracy: 0.885536\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  70 | Train Loss: 0.3534 | Test accuracy: 0.890405\n",
      "Epoch:  70 | Train Loss: 0.3534 | Train accuracy: 0.890592\n",
      "Epoch:  71 | Train Loss: 0.3450 | Test accuracy: 0.893644\n",
      "Epoch:  71 | Train Loss: 0.3450 | Train accuracy: 0.893843\n",
      "Epoch:  72 | Train Loss: 0.3358 | Test accuracy: 0.897430\n",
      "Epoch:  72 | Train Loss: 0.3358 | Train accuracy: 0.896383\n",
      "Epoch:  73 | Train Loss: 0.3290 | Test accuracy: 0.899074\n",
      "Epoch:  73 | Train Loss: 0.3290 | Train accuracy: 0.898974\n",
      "Epoch:  74 | Train Loss: 0.3235 | Test accuracy: 0.901564\n",
      "Epoch:  74 | Train Loss: 0.3235 | Train accuracy: 0.901340\n",
      "Epoch:  75 | Train Loss: 0.3187 | Test accuracy: 0.903408\n",
      "Epoch:  75 | Train Loss: 0.3187 | Train accuracy: 0.904067\n",
      "Epoch:  76 | Train Loss: 0.3023 | Test accuracy: 0.906695\n",
      "Epoch:  76 | Train Loss: 0.3023 | Train accuracy: 0.906633\n",
      "Epoch:  77 | Train Loss: 0.3011 | Test accuracy: 0.908987\n",
      "Epoch:  77 | Train Loss: 0.3011 | Train accuracy: 0.909348\n",
      "Epoch:  78 | Train Loss: 0.2890 | Test accuracy: 0.911477\n",
      "Epoch:  78 | Train Loss: 0.2890 | Train accuracy: 0.911615\n",
      "Epoch:  79 | Train Loss: 0.3162 | Test accuracy: 0.906944\n",
      "Epoch:  79 | Train Loss: 0.3162 | Train accuracy: 0.903731\n",
      "Epoch:  80 | Train Loss: 0.3801 | Test accuracy: 0.880493\n",
      "Epoch:  80 | Train Loss: 0.3801 | Train accuracy: 0.880891\n",
      "Epoch:  81 | Train Loss: 0.3244 | Test accuracy: 0.897629\n",
      "Epoch:  81 | Train Loss: 0.3244 | Train accuracy: 0.897654\n",
      "Epoch:  82 | Train Loss: 0.2988 | Test accuracy: 0.904155\n",
      "Epoch:  82 | Train Loss: 0.2988 | Train accuracy: 0.905686\n",
      "Epoch:  83 | Train Loss: 0.2781 | Test accuracy: 0.911876\n",
      "Epoch:  83 | Train Loss: 0.2781 | Train accuracy: 0.911864\n",
      "Epoch:  84 | Train Loss: 0.2750 | Test accuracy: 0.915064\n",
      "Epoch:  84 | Train Loss: 0.2750 | Train accuracy: 0.915425\n",
      "Epoch:  85 | Train Loss: 0.2695 | Test accuracy: 0.920295\n",
      "Epoch:  85 | Train Loss: 0.2695 | Train accuracy: 0.918614\n",
      "Epoch:  86 | Train Loss: 0.2549 | Test accuracy: 0.921491\n",
      "Epoch:  86 | Train Loss: 0.2549 | Train accuracy: 0.922088\n",
      "Epoch:  87 | Train Loss: 0.2531 | Test accuracy: 0.923483\n",
      "Epoch:  87 | Train Loss: 0.2531 | Train accuracy: 0.923882\n",
      "Epoch:  88 | Train Loss: 0.2522 | Test accuracy: 0.925028\n",
      "Epoch:  88 | Train Loss: 0.2522 | Train accuracy: 0.924965\n",
      "Epoch:  89 | Train Loss: 0.2505 | Test accuracy: 0.926921\n",
      "Epoch:  89 | Train Loss: 0.2505 | Train accuracy: 0.926323\n",
      "Epoch:  90 | Train Loss: 0.2335 | Test accuracy: 0.928365\n",
      "Epoch:  90 | Train Loss: 0.2335 | Train accuracy: 0.928402\n",
      "Epoch:  91 | Train Loss: 0.2295 | Test accuracy: 0.928614\n",
      "Epoch:  91 | Train Loss: 0.2295 | Train accuracy: 0.928975\n",
      "Epoch:  92 | Train Loss: 0.2353 | Test accuracy: 0.928963\n",
      "Epoch:  92 | Train Loss: 0.2353 | Train accuracy: 0.929586\n",
      "Epoch:  93 | Train Loss: 0.2216 | Test accuracy: 0.932799\n",
      "Epoch:  93 | Train Loss: 0.2216 | Train accuracy: 0.931130\n",
      "Epoch:  94 | Train Loss: 0.2186 | Test accuracy: 0.931504\n",
      "Epoch:  94 | Train Loss: 0.2186 | Train accuracy: 0.931665\n",
      "Epoch:  95 | Train Loss: 0.2440 | Test accuracy: 0.925924\n",
      "Epoch:  95 | Train Loss: 0.2440 | Train accuracy: 0.927805\n",
      "Epoch:  96 | Train Loss: 0.9507 | Test accuracy: 0.855386\n",
      "Epoch:  96 | Train Loss: 0.9507 | Train accuracy: 0.854937\n",
      "Epoch:  97 | Train Loss: 0.4777 | Test accuracy: 0.840093\n",
      "Epoch:  97 | Train Loss: 0.4777 | Train accuracy: 0.835633\n",
      "Epoch:  98 | Train Loss: 0.4208 | Test accuracy: 0.860367\n",
      "Epoch:  98 | Train Loss: 0.4208 | Train accuracy: 0.859594\n",
      "Epoch:  99 | Train Loss: 0.3595 | Test accuracy: 0.882086\n",
      "Epoch:  99 | Train Loss: 0.3595 | Train accuracy: 0.880629\n",
      "Epoch:  100 | Train Loss: 0.3044 | Test accuracy: 0.902312\n",
      "Epoch:  100 | Train Loss: 0.3044 | Train accuracy: 0.901514\n",
      "Epoch:  101 | Train Loss: 0.2689 | Test accuracy: 0.909485\n",
      "Epoch:  101 | Train Loss: 0.2689 | Train accuracy: 0.911926\n",
      "Epoch:  102 | Train Loss: 0.2477 | Test accuracy: 0.921092\n",
      "Epoch:  102 | Train Loss: 0.2477 | Train accuracy: 0.920345\n",
      "Epoch:  103 | Train Loss: 0.2340 | Test accuracy: 0.926722\n",
      "Epoch:  103 | Train Loss: 0.2340 | Train accuracy: 0.927182\n",
      "Epoch:  104 | Train Loss: 0.2192 | Test accuracy: 0.931703\n",
      "Epoch:  104 | Train Loss: 0.2192 | Train accuracy: 0.932861\n",
      "Epoch:  105 | Train Loss: 0.2066 | Test accuracy: 0.936883\n",
      "Epoch:  105 | Train Loss: 0.2066 | Train accuracy: 0.937045\n",
      "Epoch:  106 | Train Loss: 0.1991 | Test accuracy: 0.939275\n",
      "Epoch:  106 | Train Loss: 0.1991 | Train accuracy: 0.940059\n",
      "Epoch:  107 | Train Loss: 0.1952 | Test accuracy: 0.941766\n",
      "Epoch:  107 | Train Loss: 0.1952 | Train accuracy: 0.941604\n",
      "Epoch:  108 | Train Loss: 0.1887 | Test accuracy: 0.942762\n",
      "Epoch:  108 | Train Loss: 0.1887 | Train accuracy: 0.943335\n",
      "Epoch:  109 | Train Loss: 0.1863 | Test accuracy: 0.944157\n",
      "Epoch:  109 | Train Loss: 0.1863 | Train accuracy: 0.944294\n",
      "Epoch:  110 | Train Loss: 0.1800 | Test accuracy: 0.945302\n",
      "Epoch:  110 | Train Loss: 0.1800 | Train accuracy: 0.945763\n",
      "Epoch:  111 | Train Loss: 0.1771 | Test accuracy: 0.946897\n",
      "Epoch:  111 | Train Loss: 0.1771 | Train accuracy: 0.946784\n",
      "Epoch:  112 | Train Loss: 0.1736 | Test accuracy: 0.947843\n",
      "Epoch:  112 | Train Loss: 0.1736 | Train accuracy: 0.948827\n",
      "Epoch:  113 | Train Loss: 0.1702 | Test accuracy: 0.948740\n",
      "Epoch:  113 | Train Loss: 0.1702 | Train accuracy: 0.949537\n",
      "Epoch:  114 | Train Loss: 0.1678 | Test accuracy: 0.950434\n",
      "Epoch:  114 | Train Loss: 0.1678 | Train accuracy: 0.951031\n",
      "Epoch:  115 | Train Loss: 0.1628 | Test accuracy: 0.951579\n",
      "Epoch:  115 | Train Loss: 0.1628 | Train accuracy: 0.951828\n",
      "Epoch:  116 | Train Loss: 0.1632 | Test accuracy: 0.952526\n",
      "Epoch:  116 | Train Loss: 0.1632 | Train accuracy: 0.952401\n",
      "Epoch:  117 | Train Loss: 0.1543 | Test accuracy: 0.952775\n",
      "Epoch:  117 | Train Loss: 0.1543 | Train accuracy: 0.953385\n",
      "Epoch:  118 | Train Loss: 0.1527 | Test accuracy: 0.953671\n",
      "Epoch:  118 | Train Loss: 0.1527 | Train accuracy: 0.953709\n",
      "Epoch:  119 | Train Loss: 0.1537 | Test accuracy: 0.953422\n",
      "Epoch:  119 | Train Loss: 0.1537 | Train accuracy: 0.954406\n",
      "Epoch:  120 | Train Loss: 0.1523 | Test accuracy: 0.954419\n",
      "Epoch:  120 | Train Loss: 0.1523 | Train accuracy: 0.954867\n",
      "Epoch:  121 | Train Loss: 0.1481 | Test accuracy: 0.955565\n",
      "Epoch:  121 | Train Loss: 0.1481 | Train accuracy: 0.956187\n",
      "Epoch:  122 | Train Loss: 0.1476 | Test accuracy: 0.955764\n",
      "Epoch:  122 | Train Loss: 0.1476 | Train accuracy: 0.956959\n",
      "Epoch:  123 | Train Loss: 0.1510 | Test accuracy: 0.954419\n",
      "Epoch:  123 | Train Loss: 0.1510 | Train accuracy: 0.955415\n",
      "Epoch:  124 | Train Loss: 0.1457 | Test accuracy: 0.955365\n",
      "Epoch:  124 | Train Loss: 0.1457 | Train accuracy: 0.955527\n",
      "Epoch:  125 | Train Loss: 0.1390 | Test accuracy: 0.957308\n",
      "Epoch:  125 | Train Loss: 0.1390 | Train accuracy: 0.957781\n",
      "Epoch:  126 | Train Loss: 0.1334 | Test accuracy: 0.957856\n",
      "Epoch:  126 | Train Loss: 0.1334 | Train accuracy: 0.958765\n",
      "Epoch:  127 | Train Loss: 0.1313 | Test accuracy: 0.958902\n",
      "Epoch:  127 | Train Loss: 0.1313 | Train accuracy: 0.959462\n",
      "Epoch:  128 | Train Loss: 0.1284 | Test accuracy: 0.960197\n",
      "Epoch:  128 | Train Loss: 0.1284 | Train accuracy: 0.959824\n",
      "Epoch:  129 | Train Loss: 0.1295 | Test accuracy: 0.959301\n",
      "Epoch:  129 | Train Loss: 0.1295 | Train accuracy: 0.960583\n",
      "Epoch:  130 | Train Loss: 0.1285 | Test accuracy: 0.959151\n",
      "Epoch:  130 | Train Loss: 0.1285 | Train accuracy: 0.960870\n",
      "Epoch:  131 | Train Loss: 0.5899 | Test accuracy: 0.901360\n",
      "Epoch:  131 | Train Loss: 0.5899 | Train accuracy: 0.903209\n",
      "Epoch:  132 | Train Loss: 0.5295 | Test accuracy: 0.832272\n",
      "Epoch:  132 | Train Loss: 0.5295 | Train accuracy: 0.830340\n",
      "Epoch:  133 | Train Loss: 0.4014 | Test accuracy: 0.863805\n",
      "Epoch:  133 | Train Loss: 0.4014 | Train accuracy: 0.862621\n",
      "Epoch:  134 | Train Loss: 0.3239 | Test accuracy: 0.887367\n",
      "Epoch:  134 | Train Loss: 0.3239 | Train accuracy: 0.888114\n",
      "Epoch:  135 | Train Loss: 0.2657 | Test accuracy: 0.910780\n",
      "Epoch:  135 | Train Loss: 0.2657 | Train accuracy: 0.912935\n",
      "Epoch:  136 | Train Loss: 0.2240 | Test accuracy: 0.926423\n",
      "Epoch:  136 | Train Loss: 0.2240 | Train accuracy: 0.926958\n",
      "Epoch:  137 | Train Loss: 0.1889 | Test accuracy: 0.937481\n",
      "Epoch:  137 | Train Loss: 0.1889 | Train accuracy: 0.938104\n",
      "Epoch:  138 | Train Loss: 0.1680 | Test accuracy: 0.946648\n",
      "Epoch:  138 | Train Loss: 0.1680 | Train accuracy: 0.945601\n",
      "Epoch:  139 | Train Loss: 0.1508 | Test accuracy: 0.952525\n",
      "Epoch:  139 | Train Loss: 0.1508 | Train accuracy: 0.952252\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  140 | Train Loss: 0.1422 | Test accuracy: 0.954618\n",
      "Epoch:  140 | Train Loss: 0.1422 | Train accuracy: 0.955726\n",
      "Epoch:  141 | Train Loss: 0.1365 | Test accuracy: 0.957258\n",
      "Epoch:  141 | Train Loss: 0.1365 | Train accuracy: 0.958117\n",
      "Epoch:  142 | Train Loss: 0.1335 | Test accuracy: 0.960148\n",
      "Epoch:  142 | Train Loss: 0.1335 | Train accuracy: 0.959736\n",
      "Epoch:  143 | Train Loss: 0.1263 | Test accuracy: 0.960646\n",
      "Epoch:  143 | Train Loss: 0.1263 | Train accuracy: 0.960646\n",
      "Epoch:  144 | Train Loss: 0.1230 | Test accuracy: 0.961592\n",
      "Epoch:  144 | Train Loss: 0.1230 | Train accuracy: 0.961679\n",
      "Epoch:  145 | Train Loss: 0.1199 | Test accuracy: 0.961841\n",
      "Epoch:  145 | Train Loss: 0.1199 | Train accuracy: 0.962763\n",
      "Epoch:  146 | Train Loss: 0.1170 | Test accuracy: 0.962639\n",
      "Epoch:  146 | Train Loss: 0.1170 | Train accuracy: 0.963236\n",
      "Epoch:  147 | Train Loss: 0.1128 | Test accuracy: 0.963037\n",
      "Epoch:  147 | Train Loss: 0.1128 | Train accuracy: 0.963909\n",
      "Epoch:  148 | Train Loss: 0.1121 | Test accuracy: 0.963535\n",
      "Epoch:  148 | Train Loss: 0.1121 | Train accuracy: 0.964195\n",
      "Epoch:  149 | Train Loss: 0.1068 | Test accuracy: 0.964182\n",
      "Epoch:  149 | Train Loss: 0.1068 | Train accuracy: 0.964718\n",
      "Epoch:  150 | Train Loss: 0.1049 | Test accuracy: 0.963934\n",
      "Epoch:  150 | Train Loss: 0.1049 | Train accuracy: 0.965129\n",
      "Epoch:  151 | Train Loss: 0.1047 | Test accuracy: 0.964830\n",
      "Epoch:  151 | Train Loss: 0.1047 | Train accuracy: 0.965677\n",
      "Epoch:  152 | Train Loss: 0.1016 | Test accuracy: 0.965079\n",
      "Epoch:  152 | Train Loss: 0.1016 | Train accuracy: 0.965889\n",
      "Epoch:  153 | Train Loss: 0.1026 | Test accuracy: 0.965727\n",
      "Epoch:  153 | Train Loss: 0.1026 | Train accuracy: 0.966100\n",
      "Epoch:  154 | Train Loss: 0.0998 | Test accuracy: 0.965079\n",
      "Epoch:  154 | Train Loss: 0.0998 | Train accuracy: 0.966511\n",
      "Epoch:  155 | Train Loss: 0.0998 | Test accuracy: 0.965827\n",
      "Epoch:  155 | Train Loss: 0.0998 | Train accuracy: 0.966449\n",
      "Epoch:  156 | Train Loss: 0.0983 | Test accuracy: 0.965727\n",
      "Epoch:  156 | Train Loss: 0.0983 | Train accuracy: 0.966686\n",
      "Epoch:  157 | Train Loss: 0.0943 | Test accuracy: 0.965527\n",
      "Epoch:  157 | Train Loss: 0.0943 | Train accuracy: 0.966997\n",
      "Epoch:  158 | Train Loss: 0.0936 | Test accuracy: 0.966125\n",
      "Epoch:  158 | Train Loss: 0.0936 | Train accuracy: 0.967234\n",
      "Epoch:  159 | Train Loss: 0.0937 | Test accuracy: 0.966275\n",
      "Epoch:  159 | Train Loss: 0.0937 | Train accuracy: 0.967520\n",
      "Epoch:  160 | Train Loss: 0.0949 | Test accuracy: 0.967321\n",
      "Epoch:  160 | Train Loss: 0.0949 | Train accuracy: 0.967695\n",
      "Epoch:  161 | Train Loss: 0.0903 | Test accuracy: 0.966374\n",
      "Epoch:  161 | Train Loss: 0.0903 | Train accuracy: 0.967782\n",
      "Epoch:  162 | Train Loss: 0.0894 | Test accuracy: 0.967271\n",
      "Epoch:  162 | Train Loss: 0.0894 | Train accuracy: 0.968255\n",
      "Epoch:  163 | Train Loss: 0.0880 | Test accuracy: 0.966723\n",
      "Epoch:  163 | Train Loss: 0.0880 | Train accuracy: 0.968392\n",
      "Epoch:  164 | Train Loss: 0.0878 | Test accuracy: 0.966723\n",
      "Epoch:  164 | Train Loss: 0.0878 | Train accuracy: 0.968516\n",
      "Epoch:  165 | Train Loss: 0.0875 | Test accuracy: 0.967670\n",
      "Epoch:  165 | Train Loss: 0.0875 | Train accuracy: 0.968492\n",
      "Epoch:  166 | Train Loss: 0.0862 | Test accuracy: 0.966424\n",
      "Epoch:  166 | Train Loss: 0.0862 | Train accuracy: 0.968840\n",
      "Epoch:  167 | Train Loss: 0.0849 | Test accuracy: 0.968766\n",
      "Epoch:  167 | Train Loss: 0.0849 | Train accuracy: 0.968404\n",
      "Epoch:  168 | Train Loss: 0.0836 | Test accuracy: 0.967620\n",
      "Epoch:  168 | Train Loss: 0.0836 | Train accuracy: 0.968940\n",
      "Epoch:  169 | Train Loss: 0.0813 | Test accuracy: 0.968018\n",
      "Epoch:  169 | Train Loss: 0.0813 | Train accuracy: 0.968828\n",
      "Epoch:  170 | Train Loss: 0.0849 | Test accuracy: 0.968716\n",
      "Epoch:  170 | Train Loss: 0.0849 | Train accuracy: 0.968865\n",
      "Epoch:  171 | Train Loss: 0.0821 | Test accuracy: 0.966823\n",
      "Epoch:  171 | Train Loss: 0.0821 | Train accuracy: 0.969401\n",
      "Epoch:  172 | Train Loss: 0.0820 | Test accuracy: 0.967919\n",
      "Epoch:  172 | Train Loss: 0.0820 | Train accuracy: 0.969201\n",
      "Epoch:  173 | Train Loss: 0.0813 | Test accuracy: 0.968367\n",
      "Epoch:  173 | Train Loss: 0.0813 | Train accuracy: 0.969214\n",
      "Epoch:  174 | Train Loss: 0.0801 | Test accuracy: 0.968467\n",
      "Epoch:  174 | Train Loss: 0.0801 | Train accuracy: 0.969164\n",
      "Epoch:  175 | Train Loss: 0.0769 | Test accuracy: 0.968267\n",
      "Epoch:  175 | Train Loss: 0.0769 | Train accuracy: 0.969413\n",
      "Epoch:  176 | Train Loss: 0.0803 | Test accuracy: 0.968268\n",
      "Epoch:  176 | Train Loss: 0.0803 | Train accuracy: 0.969513\n",
      "Epoch:  177 | Train Loss: 0.0766 | Test accuracy: 0.968566\n",
      "Epoch:  177 | Train Loss: 0.0766 | Train accuracy: 0.969451\n",
      "Epoch:  178 | Train Loss: 0.0805 | Test accuracy: 0.967919\n",
      "Epoch:  178 | Train Loss: 0.0805 | Train accuracy: 0.969625\n",
      "Epoch:  179 | Train Loss: 0.0784 | Test accuracy: 0.968965\n",
      "Epoch:  179 | Train Loss: 0.0784 | Train accuracy: 0.969388\n",
      "Epoch:  180 | Train Loss: 0.0746 | Test accuracy: 0.968317\n",
      "Epoch:  180 | Train Loss: 0.0746 | Train accuracy: 0.969812\n",
      "Epoch:  181 | Train Loss: 0.0773 | Test accuracy: 0.968766\n",
      "Epoch:  181 | Train Loss: 0.0773 | Train accuracy: 0.969712\n",
      "Epoch:  182 | Train Loss: 0.0737 | Test accuracy: 0.968467\n",
      "Epoch:  182 | Train Loss: 0.0737 | Train accuracy: 0.969837\n",
      "Epoch:  183 | Train Loss: 0.0738 | Test accuracy: 0.968517\n",
      "Epoch:  183 | Train Loss: 0.0738 | Train accuracy: 0.969961\n",
      "Epoch:  184 | Train Loss: 0.0733 | Test accuracy: 0.967869\n",
      "Epoch:  184 | Train Loss: 0.0733 | Train accuracy: 0.970111\n",
      "Epoch:  185 | Train Loss: 0.0716 | Test accuracy: 0.968915\n",
      "Epoch:  185 | Train Loss: 0.0716 | Train accuracy: 0.970036\n",
      "Epoch:  186 | Train Loss: 0.0717 | Test accuracy: 0.969214\n",
      "Epoch:  186 | Train Loss: 0.0717 | Train accuracy: 0.969974\n",
      "Epoch:  187 | Train Loss: 0.0699 | Test accuracy: 0.969313\n",
      "Epoch:  187 | Train Loss: 0.0699 | Train accuracy: 0.969936\n",
      "Epoch:  188 | Train Loss: 0.0722 | Test accuracy: 0.969164\n",
      "Epoch:  188 | Train Loss: 0.0722 | Train accuracy: 0.969911\n",
      "Epoch:  189 | Train Loss: 0.0696 | Test accuracy: 0.969363\n",
      "Epoch:  189 | Train Loss: 0.0696 | Train accuracy: 0.970023\n",
      "Epoch:  190 | Train Loss: 0.0685 | Test accuracy: 0.969065\n",
      "Epoch:  190 | Train Loss: 0.0685 | Train accuracy: 0.970023\n",
      "Epoch:  191 | Train Loss: 0.0694 | Test accuracy: 0.968816\n",
      "Epoch:  191 | Train Loss: 0.0694 | Train accuracy: 0.970385\n",
      "Epoch:  192 | Train Loss: 0.0693 | Test accuracy: 0.968616\n",
      "Epoch:  192 | Train Loss: 0.0693 | Train accuracy: 0.970459\n",
      "Epoch:  193 | Train Loss: 0.0684 | Test accuracy: 0.968168\n",
      "Epoch:  193 | Train Loss: 0.0684 | Train accuracy: 0.970708\n",
      "Epoch:  194 | Train Loss: 0.0674 | Test accuracy: 0.969314\n",
      "Epoch:  194 | Train Loss: 0.0674 | Train accuracy: 0.970509\n",
      "Epoch:  195 | Train Loss: 0.0678 | Test accuracy: 0.969363\n",
      "Epoch:  195 | Train Loss: 0.0678 | Train accuracy: 0.970534\n",
      "Epoch:  196 | Train Loss: 0.0682 | Test accuracy: 0.969413\n",
      "Epoch:  196 | Train Loss: 0.0682 | Train accuracy: 0.970509\n",
      "Epoch:  197 | Train Loss: 0.0688 | Test accuracy: 0.969961\n",
      "Epoch:  197 | Train Loss: 0.0688 | Train accuracy: 0.970509\n",
      "Epoch:  198 | Train Loss: 0.0669 | Test accuracy: 0.970111\n",
      "Epoch:  198 | Train Loss: 0.0669 | Train accuracy: 0.970459\n",
      "Epoch:  199 | Train Loss: 0.0661 | Test accuracy: 0.968417\n",
      "Epoch:  199 | Train Loss: 0.0661 | Train accuracy: 0.970920\n",
      "Epoch:  200 | Train Loss: 0.0655 | Test accuracy: 0.968766\n",
      "Epoch:  200 | Train Loss: 0.0655 | Train accuracy: 0.970808\n",
      "Epoch:  201 | Train Loss: 0.0640 | Test accuracy: 0.969065\n",
      "Epoch:  201 | Train Loss: 0.0640 | Train accuracy: 0.970833\n",
      "Epoch:  202 | Train Loss: 0.0677 | Test accuracy: 0.969762\n",
      "Epoch:  202 | Train Loss: 0.0677 | Train accuracy: 0.970459\n",
      "Epoch:  203 | Train Loss: 0.0651 | Test accuracy: 0.968616\n",
      "Epoch:  203 | Train Loss: 0.0651 | Train accuracy: 0.970970\n",
      "Epoch:  204 | Train Loss: 0.0666 | Test accuracy: 0.969364\n",
      "Epoch:  204 | Train Loss: 0.0666 | Train accuracy: 0.970746\n",
      "Epoch:  205 | Train Loss: 0.0651 | Test accuracy: 0.969065\n",
      "Epoch:  205 | Train Loss: 0.0651 | Train accuracy: 0.970758\n",
      "Epoch:  206 | Train Loss: 0.0649 | Test accuracy: 0.969762\n",
      "Epoch:  206 | Train Loss: 0.0649 | Train accuracy: 0.970671\n",
      "Epoch:  207 | Train Loss: 0.0643 | Test accuracy: 0.968766\n",
      "Epoch:  207 | Train Loss: 0.0643 | Train accuracy: 0.971007\n",
      "Epoch:  208 | Train Loss: 0.0638 | Test accuracy: 0.968915\n",
      "Epoch:  208 | Train Loss: 0.0638 | Train accuracy: 0.970982\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  209 | Train Loss: 0.0636 | Test accuracy: 0.969812\n",
      "Epoch:  209 | Train Loss: 0.0636 | Train accuracy: 0.970808\n",
      "Epoch:  210 | Train Loss: 0.0612 | Test accuracy: 0.969413\n",
      "Epoch:  210 | Train Loss: 0.0612 | Train accuracy: 0.970933\n",
      "Epoch:  211 | Train Loss: 0.0638 | Test accuracy: 0.968367\n",
      "Epoch:  211 | Train Loss: 0.0638 | Train accuracy: 0.971107\n",
      "Epoch:  212 | Train Loss: 0.0634 | Test accuracy: 0.969463\n",
      "Epoch:  212 | Train Loss: 0.0634 | Train accuracy: 0.970920\n",
      "Epoch:  213 | Train Loss: 0.0607 | Test accuracy: 0.969812\n",
      "Epoch:  213 | Train Loss: 0.0607 | Train accuracy: 0.970895\n",
      "Epoch:  214 | Train Loss: 0.0620 | Test accuracy: 0.969563\n",
      "Epoch:  214 | Train Loss: 0.0620 | Train accuracy: 0.970908\n",
      "Epoch:  215 | Train Loss: 0.0625 | Test accuracy: 0.969812\n",
      "Epoch:  215 | Train Loss: 0.0625 | Train accuracy: 0.970870\n",
      "Epoch:  216 | Train Loss: 0.0600 | Test accuracy: 0.969812\n",
      "Epoch:  216 | Train Loss: 0.0600 | Train accuracy: 0.970820\n",
      "Epoch:  217 | Train Loss: 0.0631 | Test accuracy: 0.969911\n",
      "Epoch:  217 | Train Loss: 0.0631 | Train accuracy: 0.970733\n",
      "Epoch:  218 | Train Loss: 0.0653 | Test accuracy: 0.969214\n",
      "Epoch:  218 | Train Loss: 0.0653 | Train accuracy: 0.970995\n",
      "Epoch:  219 | Train Loss: 0.0603 | Test accuracy: 0.968965\n",
      "Epoch:  219 | Train Loss: 0.0603 | Train accuracy: 0.971070\n",
      "Epoch:  220 | Train Loss: 0.0611 | Test accuracy: 0.969712\n",
      "Epoch:  220 | Train Loss: 0.0611 | Train accuracy: 0.970883\n",
      "Epoch:  221 | Train Loss: 0.0616 | Test accuracy: 0.969912\n",
      "Epoch:  221 | Train Loss: 0.0616 | Train accuracy: 0.970945\n",
      "Epoch:  222 | Train Loss: 0.0595 | Test accuracy: 0.969164\n",
      "Epoch:  222 | Train Loss: 0.0595 | Train accuracy: 0.971144\n",
      "Epoch:  223 | Train Loss: 0.0605 | Test accuracy: 0.969613\n",
      "Epoch:  223 | Train Loss: 0.0605 | Train accuracy: 0.971020\n",
      "Epoch:  224 | Train Loss: 0.0588 | Test accuracy: 0.968965\n",
      "Epoch:  224 | Train Loss: 0.0588 | Train accuracy: 0.971132\n",
      "Epoch:  225 | Train Loss: 0.0593 | Test accuracy: 0.969662\n",
      "Epoch:  225 | Train Loss: 0.0593 | Train accuracy: 0.971057\n",
      "Epoch:  226 | Train Loss: 0.0588 | Test accuracy: 0.968716\n",
      "Epoch:  226 | Train Loss: 0.0588 | Train accuracy: 0.971306\n",
      "Epoch:  227 | Train Loss: 0.0582 | Test accuracy: 0.968815\n",
      "Epoch:  227 | Train Loss: 0.0582 | Train accuracy: 0.971256\n",
      "Epoch:  228 | Train Loss: 0.0586 | Test accuracy: 0.969065\n",
      "Epoch:  228 | Train Loss: 0.0586 | Train accuracy: 0.971207\n",
      "Epoch:  229 | Train Loss: 0.0592 | Test accuracy: 0.970609\n",
      "Epoch:  229 | Train Loss: 0.0592 | Train accuracy: 0.970820\n",
      "Epoch:  230 | Train Loss: 0.0592 | Test accuracy: 0.969712\n",
      "Epoch:  230 | Train Loss: 0.0592 | Train accuracy: 0.971169\n",
      "Epoch:  231 | Train Loss: 0.0585 | Test accuracy: 0.970808\n",
      "Epoch:  231 | Train Loss: 0.0585 | Train accuracy: 0.970908\n",
      "Epoch:  232 | Train Loss: 0.0563 | Test accuracy: 0.969314\n",
      "Epoch:  232 | Train Loss: 0.0563 | Train accuracy: 0.971256\n",
      "Epoch:  233 | Train Loss: 0.0595 | Test accuracy: 0.970260\n",
      "Epoch:  233 | Train Loss: 0.0595 | Train accuracy: 0.970957\n",
      "Epoch:  234 | Train Loss: 0.0591 | Test accuracy: 0.970310\n",
      "Epoch:  234 | Train Loss: 0.0591 | Train accuracy: 0.970982\n",
      "Epoch:  235 | Train Loss: 0.0578 | Test accuracy: 0.968865\n",
      "Epoch:  235 | Train Loss: 0.0578 | Train accuracy: 0.971306\n",
      "Epoch:  236 | Train Loss: 0.0600 | Test accuracy: 0.969612\n",
      "Epoch:  236 | Train Loss: 0.0600 | Train accuracy: 0.971219\n",
      "Epoch:  237 | Train Loss: 0.0582 | Test accuracy: 0.969663\n",
      "Epoch:  237 | Train Loss: 0.0582 | Train accuracy: 0.971194\n",
      "Epoch:  238 | Train Loss: 0.0573 | Test accuracy: 0.968965\n",
      "Epoch:  238 | Train Loss: 0.0573 | Train accuracy: 0.971393\n",
      "Epoch:  239 | Train Loss: 0.0568 | Test accuracy: 0.969762\n",
      "Epoch:  239 | Train Loss: 0.0568 | Train accuracy: 0.971157\n",
      "Epoch:  240 | Train Loss: 0.0588 | Test accuracy: 0.969114\n",
      "Epoch:  240 | Train Loss: 0.0588 | Train accuracy: 0.971356\n",
      "Epoch:  241 | Train Loss: 0.0583 | Test accuracy: 0.970758\n",
      "Epoch:  241 | Train Loss: 0.0583 | Train accuracy: 0.970908\n",
      "Epoch:  242 | Train Loss: 0.0588 | Test accuracy: 0.970210\n",
      "Epoch:  242 | Train Loss: 0.0588 | Train accuracy: 0.971020\n",
      "Epoch:  243 | Train Loss: 0.0565 | Test accuracy: 0.969862\n",
      "Epoch:  243 | Train Loss: 0.0565 | Train accuracy: 0.971144\n",
      "Epoch:  244 | Train Loss: 0.0559 | Test accuracy: 0.969812\n",
      "Epoch:  244 | Train Loss: 0.0559 | Train accuracy: 0.971169\n",
      "Epoch:  245 | Train Loss: 0.0565 | Test accuracy: 0.969015\n",
      "Epoch:  245 | Train Loss: 0.0565 | Train accuracy: 0.971319\n",
      "Epoch:  246 | Train Loss: 0.0583 | Test accuracy: 0.969414\n",
      "Epoch:  246 | Train Loss: 0.0583 | Train accuracy: 0.971294\n",
      "Epoch:  247 | Train Loss: 0.0563 | Test accuracy: 0.969862\n",
      "Epoch:  247 | Train Loss: 0.0563 | Train accuracy: 0.971169\n",
      "Epoch:  248 | Train Loss: 0.0557 | Test accuracy: 0.969563\n",
      "Epoch:  248 | Train Loss: 0.0557 | Train accuracy: 0.971281\n",
      "Epoch:  249 | Train Loss: 0.0563 | Test accuracy: 0.969613\n",
      "Epoch:  249 | Train Loss: 0.0563 | Train accuracy: 0.971194\n",
      "Epoch:  250 | Train Loss: 0.0552 | Test accuracy: 0.969065\n",
      "Epoch:  250 | Train Loss: 0.0552 | Train accuracy: 0.971393\n",
      "Epoch:  251 | Train Loss: 0.0571 | Test accuracy: 0.969413\n",
      "Epoch:  251 | Train Loss: 0.0571 | Train accuracy: 0.971219\n",
      "Epoch:  252 | Train Loss: 0.0561 | Test accuracy: 0.969912\n",
      "Epoch:  252 | Train Loss: 0.0561 | Train accuracy: 0.971157\n",
      "Epoch:  253 | Train Loss: 0.0547 | Test accuracy: 0.969363\n",
      "Epoch:  253 | Train Loss: 0.0547 | Train accuracy: 0.971256\n",
      "Epoch:  254 | Train Loss: 0.0565 | Test accuracy: 0.969612\n",
      "Epoch:  254 | Train Loss: 0.0565 | Train accuracy: 0.971194\n",
      "Epoch:  255 | Train Loss: 0.0561 | Test accuracy: 0.971157\n",
      "Epoch:  255 | Train Loss: 0.0561 | Train accuracy: 0.970858\n",
      "Epoch:  256 | Train Loss: 0.0540 | Test accuracy: 0.967919\n",
      "Epoch:  256 | Train Loss: 0.0540 | Train accuracy: 0.971605\n",
      "Epoch:  257 | Train Loss: 0.0574 | Test accuracy: 0.970310\n",
      "Epoch:  257 | Train Loss: 0.0574 | Train accuracy: 0.970970\n",
      "Epoch:  258 | Train Loss: 0.0544 | Test accuracy: 0.969214\n",
      "Epoch:  258 | Train Loss: 0.0544 | Train accuracy: 0.971281\n",
      "Epoch:  259 | Train Loss: 0.0547 | Test accuracy: 0.968666\n",
      "Epoch:  259 | Train Loss: 0.0547 | Train accuracy: 0.971443\n",
      "Epoch:  260 | Train Loss: 0.0575 | Test accuracy: 0.969613\n",
      "Epoch:  260 | Train Loss: 0.0575 | Train accuracy: 0.971144\n",
      "Epoch:  261 | Train Loss: 0.0569 | Test accuracy: 0.969662\n",
      "Epoch:  261 | Train Loss: 0.0569 | Train accuracy: 0.971169\n",
      "Epoch:  262 | Train Loss: 0.0558 | Test accuracy: 0.968766\n",
      "Epoch:  262 | Train Loss: 0.0558 | Train accuracy: 0.971393\n",
      "Epoch:  263 | Train Loss: 0.0546 | Test accuracy: 0.969513\n",
      "Epoch:  263 | Train Loss: 0.0546 | Train accuracy: 0.971231\n",
      "Epoch:  264 | Train Loss: 0.0539 | Test accuracy: 0.969613\n",
      "Epoch:  264 | Train Loss: 0.0539 | Train accuracy: 0.971144\n",
      "Epoch:  265 | Train Loss: 0.0545 | Test accuracy: 0.969463\n",
      "Epoch:  265 | Train Loss: 0.0545 | Train accuracy: 0.971219\n",
      "Epoch:  266 | Train Loss: 0.0542 | Test accuracy: 0.968716\n",
      "Epoch:  266 | Train Loss: 0.0542 | Train accuracy: 0.971468\n",
      "Epoch:  267 | Train Loss: 0.0525 | Test accuracy: 0.969114\n",
      "Epoch:  267 | Train Loss: 0.0525 | Train accuracy: 0.971344\n",
      "Epoch:  268 | Train Loss: 0.0533 | Test accuracy: 0.968915\n",
      "Epoch:  268 | Train Loss: 0.0533 | Train accuracy: 0.971393\n",
      "Epoch:  269 | Train Loss: 0.0548 | Test accuracy: 0.970210\n",
      "Epoch:  269 | Train Loss: 0.0548 | Train accuracy: 0.971082\n",
      "Epoch:  270 | Train Loss: 0.0521 | Test accuracy: 0.969762\n",
      "Epoch:  270 | Train Loss: 0.0521 | Train accuracy: 0.971194\n",
      "Epoch:  271 | Train Loss: 0.0540 | Test accuracy: 0.969214\n",
      "Epoch:  271 | Train Loss: 0.0540 | Train accuracy: 0.971344\n",
      "Epoch:  272 | Train Loss: 0.0523 | Test accuracy: 0.970111\n",
      "Epoch:  272 | Train Loss: 0.0523 | Train accuracy: 0.971020\n",
      "Epoch:  273 | Train Loss: 0.0535 | Test accuracy: 0.969612\n",
      "Epoch:  273 | Train Loss: 0.0535 | Train accuracy: 0.971207\n",
      "Epoch:  274 | Train Loss: 0.0526 | Test accuracy: 0.970609\n",
      "Epoch:  274 | Train Loss: 0.0526 | Train accuracy: 0.970995\n",
      "Epoch:  275 | Train Loss: 0.0522 | Test accuracy: 0.969712\n",
      "Epoch:  275 | Train Loss: 0.0522 | Train accuracy: 0.971094\n",
      "Epoch:  276 | Train Loss: 0.0550 | Test accuracy: 0.968915\n",
      "Epoch:  276 | Train Loss: 0.0550 | Train accuracy: 0.971381\n",
      "Epoch:  277 | Train Loss: 0.0550 | Test accuracy: 0.969513\n",
      "Epoch:  277 | Train Loss: 0.0550 | Train accuracy: 0.971281\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  278 | Train Loss: 0.0554 | Test accuracy: 0.969164\n",
      "Epoch:  278 | Train Loss: 0.0554 | Train accuracy: 0.971294\n",
      "Epoch:  279 | Train Loss: 0.0544 | Test accuracy: 0.969662\n",
      "Epoch:  279 | Train Loss: 0.0544 | Train accuracy: 0.971157\n",
      "Epoch:  280 | Train Loss: 0.0535 | Test accuracy: 0.970111\n",
      "Epoch:  280 | Train Loss: 0.0535 | Train accuracy: 0.971107\n",
      "Epoch:  281 | Train Loss: 0.0520 | Test accuracy: 0.970061\n",
      "Epoch:  281 | Train Loss: 0.0520 | Train accuracy: 0.971157\n",
      "Epoch:  282 | Train Loss: 0.0523 | Test accuracy: 0.968118\n",
      "Epoch:  282 | Train Loss: 0.0523 | Train accuracy: 0.971518\n",
      "Epoch:  283 | Train Loss: 0.0537 | Test accuracy: 0.968268\n",
      "Epoch:  283 | Train Loss: 0.0537 | Train accuracy: 0.971580\n",
      "Epoch:  284 | Train Loss: 0.0521 | Test accuracy: 0.969413\n",
      "Epoch:  284 | Train Loss: 0.0521 | Train accuracy: 0.971281\n",
      "Epoch:  285 | Train Loss: 0.0535 | Test accuracy: 0.969712\n",
      "Epoch:  285 | Train Loss: 0.0535 | Train accuracy: 0.971219\n",
      "Epoch:  286 | Train Loss: 0.0530 | Test accuracy: 0.969264\n",
      "Epoch:  286 | Train Loss: 0.0530 | Train accuracy: 0.971306\n",
      "Epoch:  287 | Train Loss: 0.0522 | Test accuracy: 0.969762\n",
      "Epoch:  287 | Train Loss: 0.0522 | Train accuracy: 0.971169\n",
      "Epoch:  288 | Train Loss: 0.0526 | Test accuracy: 0.969911\n",
      "Epoch:  288 | Train Loss: 0.0526 | Train accuracy: 0.971144\n",
      "Epoch:  289 | Train Loss: 0.0528 | Test accuracy: 0.968666\n",
      "Epoch:  289 | Train Loss: 0.0528 | Train accuracy: 0.971456\n",
      "Epoch:  290 | Train Loss: 0.0523 | Test accuracy: 0.969463\n",
      "Epoch:  290 | Train Loss: 0.0523 | Train accuracy: 0.971231\n",
      "Epoch:  291 | Train Loss: 0.0528 | Test accuracy: 0.969463\n",
      "Epoch:  291 | Train Loss: 0.0528 | Train accuracy: 0.971269\n",
      "Epoch:  292 | Train Loss: 0.0519 | Test accuracy: 0.969264\n",
      "Epoch:  292 | Train Loss: 0.0519 | Train accuracy: 0.971281\n",
      "Epoch:  293 | Train Loss: 0.0535 | Test accuracy: 0.970310\n",
      "Epoch:  293 | Train Loss: 0.0535 | Train accuracy: 0.971007\n",
      "Epoch:  294 | Train Loss: 0.0527 | Test accuracy: 0.969314\n",
      "Epoch:  294 | Train Loss: 0.0527 | Train accuracy: 0.971244\n",
      "Epoch:  295 | Train Loss: 0.0526 | Test accuracy: 0.969264\n",
      "Epoch:  295 | Train Loss: 0.0526 | Train accuracy: 0.971331\n",
      "Epoch:  296 | Train Loss: 0.0535 | Test accuracy: 0.969663\n",
      "Epoch:  296 | Train Loss: 0.0535 | Train accuracy: 0.971219\n",
      "Epoch:  297 | Train Loss: 0.0521 | Test accuracy: 0.968915\n",
      "Epoch:  297 | Train Loss: 0.0521 | Train accuracy: 0.971406\n",
      "Epoch:  298 | Train Loss: 0.0511 | Test accuracy: 0.969662\n",
      "Epoch:  298 | Train Loss: 0.0511 | Train accuracy: 0.971182\n",
      "Epoch:  299 | Train Loss: 0.0532 | Test accuracy: 0.970609\n",
      "Epoch:  299 | Train Loss: 0.0532 | Train accuracy: 0.970945\n",
      "Epoch:  300 | Train Loss: 0.0544 | Test accuracy: 0.968118\n",
      "Epoch:  300 | Train Loss: 0.0544 | Train accuracy: 0.971618\n",
      "Epoch:  301 | Train Loss: 0.0535 | Test accuracy: 0.970410\n",
      "Epoch:  301 | Train Loss: 0.0535 | Train accuracy: 0.971032\n",
      "Epoch:  302 | Train Loss: 0.0520 | Test accuracy: 0.970061\n",
      "Epoch:  302 | Train Loss: 0.0520 | Train accuracy: 0.971107\n",
      "Epoch:  303 | Train Loss: 0.0537 | Test accuracy: 0.969015\n",
      "Epoch:  303 | Train Loss: 0.0537 | Train accuracy: 0.971306\n",
      "Epoch:  304 | Train Loss: 0.0544 | Test accuracy: 0.969413\n",
      "Epoch:  304 | Train Loss: 0.0544 | Train accuracy: 0.971207\n",
      "Epoch:  305 | Train Loss: 0.0516 | Test accuracy: 0.969712\n",
      "Epoch:  305 | Train Loss: 0.0516 | Train accuracy: 0.971169\n",
      "Epoch:  306 | Train Loss: 0.0535 | Test accuracy: 0.969065\n",
      "Epoch:  306 | Train Loss: 0.0535 | Train accuracy: 0.971294\n",
      "Epoch:  307 | Train Loss: 0.0524 | Test accuracy: 0.969513\n",
      "Epoch:  307 | Train Loss: 0.0524 | Train accuracy: 0.971194\n",
      "Epoch:  308 | Train Loss: 0.0524 | Test accuracy: 0.970758\n",
      "Epoch:  308 | Train Loss: 0.0524 | Train accuracy: 0.970920\n",
      "Epoch:  309 | Train Loss: 0.0530 | Test accuracy: 0.969463\n",
      "Epoch:  309 | Train Loss: 0.0530 | Train accuracy: 0.971269\n",
      "Epoch:  310 | Train Loss: 0.0522 | Test accuracy: 0.969712\n",
      "Epoch:  310 | Train Loss: 0.0522 | Train accuracy: 0.971219\n",
      "Epoch:  311 | Train Loss: 0.0528 | Test accuracy: 0.970609\n",
      "Epoch:  311 | Train Loss: 0.0528 | Train accuracy: 0.970995\n",
      "Epoch:  312 | Train Loss: 0.0515 | Test accuracy: 0.968766\n",
      "Epoch:  312 | Train Loss: 0.0515 | Train accuracy: 0.971481\n",
      "Epoch:  313 | Train Loss: 0.0519 | Test accuracy: 0.970011\n",
      "Epoch:  313 | Train Loss: 0.0519 | Train accuracy: 0.971144\n",
      "Epoch:  314 | Train Loss: 0.0510 | Test accuracy: 0.969862\n",
      "Epoch:  314 | Train Loss: 0.0510 | Train accuracy: 0.971207\n",
      "Epoch:  315 | Train Loss: 0.0521 | Test accuracy: 0.969613\n",
      "Epoch:  315 | Train Loss: 0.0521 | Train accuracy: 0.971219\n",
      "Epoch:  316 | Train Loss: 0.0517 | Test accuracy: 0.969214\n",
      "Epoch:  316 | Train Loss: 0.0517 | Train accuracy: 0.971294\n",
      "Epoch:  317 | Train Loss: 0.0515 | Test accuracy: 0.969314\n",
      "Epoch:  317 | Train Loss: 0.0515 | Train accuracy: 0.971306\n",
      "Epoch:  318 | Train Loss: 0.0511 | Test accuracy: 0.969911\n",
      "Epoch:  318 | Train Loss: 0.0511 | Train accuracy: 0.971119\n",
      "Epoch:  319 | Train Loss: 0.0514 | Test accuracy: 0.969862\n",
      "Epoch:  319 | Train Loss: 0.0514 | Train accuracy: 0.971207\n",
      "Epoch:  320 | Train Loss: 0.0493 | Test accuracy: 0.969563\n",
      "Epoch:  320 | Train Loss: 0.0493 | Train accuracy: 0.971244\n",
      "Epoch:  321 | Train Loss: 0.0506 | Test accuracy: 0.969114\n",
      "Epoch:  321 | Train Loss: 0.0506 | Train accuracy: 0.971306\n",
      "Epoch:  322 | Train Loss: 0.0495 | Test accuracy: 0.970111\n",
      "Epoch:  322 | Train Loss: 0.0495 | Train accuracy: 0.971082\n",
      "Epoch:  323 | Train Loss: 0.0512 | Test accuracy: 0.970559\n",
      "Epoch:  323 | Train Loss: 0.0512 | Train accuracy: 0.970970\n",
      "Epoch:  324 | Train Loss: 0.0526 | Test accuracy: 0.968317\n",
      "Epoch:  324 | Train Loss: 0.0526 | Train accuracy: 0.971543\n",
      "Epoch:  325 | Train Loss: 0.0509 | Test accuracy: 0.969912\n",
      "Epoch:  325 | Train Loss: 0.0509 | Train accuracy: 0.971132\n",
      "Epoch:  326 | Train Loss: 0.0523 | Test accuracy: 0.969214\n",
      "Epoch:  326 | Train Loss: 0.0523 | Train accuracy: 0.971319\n",
      "Epoch:  327 | Train Loss: 0.0514 | Test accuracy: 0.969463\n",
      "Epoch:  327 | Train Loss: 0.0514 | Train accuracy: 0.971281\n",
      "Epoch:  328 | Train Loss: 0.0524 | Test accuracy: 0.969364\n",
      "Epoch:  328 | Train Loss: 0.0524 | Train accuracy: 0.971331\n",
      "Epoch:  329 | Train Loss: 0.0504 | Test accuracy: 0.969961\n",
      "Epoch:  329 | Train Loss: 0.0504 | Train accuracy: 0.971144\n",
      "Epoch:  330 | Train Loss: 0.0518 | Test accuracy: 0.968218\n",
      "Epoch:  330 | Train Loss: 0.0518 | Train accuracy: 0.971593\n",
      "Epoch:  331 | Train Loss: 0.0529 | Test accuracy: 0.969513\n",
      "Epoch:  331 | Train Loss: 0.0529 | Train accuracy: 0.971207\n",
      "Epoch:  332 | Train Loss: 0.0520 | Test accuracy: 0.969214\n",
      "Epoch:  332 | Train Loss: 0.0520 | Train accuracy: 0.971269\n",
      "Epoch:  333 | Train Loss: 0.0524 | Test accuracy: 0.969513\n",
      "Epoch:  333 | Train Loss: 0.0524 | Train accuracy: 0.971231\n",
      "Epoch:  334 | Train Loss: 0.0515 | Test accuracy: 0.968816\n",
      "Epoch:  334 | Train Loss: 0.0515 | Train accuracy: 0.971381\n",
      "Epoch:  335 | Train Loss: 0.0507 | Test accuracy: 0.969563\n",
      "Epoch:  335 | Train Loss: 0.0507 | Train accuracy: 0.971244\n",
      "Epoch:  336 | Train Loss: 0.0497 | Test accuracy: 0.969662\n",
      "Epoch:  336 | Train Loss: 0.0497 | Train accuracy: 0.971294\n",
      "Epoch:  337 | Train Loss: 0.0519 | Test accuracy: 0.969413\n",
      "Epoch:  337 | Train Loss: 0.0519 | Train accuracy: 0.971294\n",
      "Epoch:  338 | Train Loss: 0.0512 | Test accuracy: 0.969513\n",
      "Epoch:  338 | Train Loss: 0.0512 | Train accuracy: 0.971331\n",
      "Epoch:  339 | Train Loss: 0.0504 | Test accuracy: 0.969364\n",
      "Epoch:  339 | Train Loss: 0.0504 | Train accuracy: 0.971356\n",
      "Epoch:  340 | Train Loss: 0.0508 | Test accuracy: 0.969114\n",
      "Epoch:  340 | Train Loss: 0.0508 | Train accuracy: 0.971456\n",
      "Epoch:  341 | Train Loss: 0.0509 | Test accuracy: 0.968616\n",
      "Epoch:  341 | Train Loss: 0.0509 | Train accuracy: 0.971530\n",
      "Epoch:  342 | Train Loss: 0.0510 | Test accuracy: 0.969911\n",
      "Epoch:  342 | Train Loss: 0.0510 | Train accuracy: 0.971169\n",
      "Epoch:  343 | Train Loss: 0.0512 | Test accuracy: 0.969762\n",
      "Epoch:  343 | Train Loss: 0.0512 | Train accuracy: 0.971256\n",
      "Epoch:  344 | Train Loss: 0.0497 | Test accuracy: 0.968516\n",
      "Epoch:  344 | Train Loss: 0.0497 | Train accuracy: 0.971481\n",
      "Epoch:  345 | Train Loss: 0.0513 | Test accuracy: 0.970061\n",
      "Epoch:  345 | Train Loss: 0.0513 | Train accuracy: 0.971169\n",
      "Epoch:  346 | Train Loss: 0.0514 | Test accuracy: 0.970061\n",
      "Epoch:  346 | Train Loss: 0.0514 | Train accuracy: 0.971144\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  347 | Train Loss: 0.0514 | Test accuracy: 0.968566\n",
      "Epoch:  347 | Train Loss: 0.0514 | Train accuracy: 0.971555\n",
      "Epoch:  348 | Train Loss: 0.0522 | Test accuracy: 0.969712\n",
      "Epoch:  348 | Train Loss: 0.0522 | Train accuracy: 0.971244\n",
      "Epoch:  349 | Train Loss: 0.0497 | Test accuracy: 0.968965\n",
      "Epoch:  349 | Train Loss: 0.0497 | Train accuracy: 0.971468\n",
      "Epoch:  350 | Train Loss: 0.0516 | Test accuracy: 0.969712\n",
      "Epoch:  350 | Train Loss: 0.0516 | Train accuracy: 0.971219\n",
      "Epoch:  351 | Train Loss: 0.0515 | Test accuracy: 0.968417\n",
      "Epoch:  351 | Train Loss: 0.0515 | Train accuracy: 0.971593\n",
      "Epoch:  352 | Train Loss: 0.0514 | Test accuracy: 0.969613\n",
      "Epoch:  352 | Train Loss: 0.0514 | Train accuracy: 0.971207\n",
      "Epoch:  353 | Train Loss: 0.0516 | Test accuracy: 0.969264\n",
      "Epoch:  353 | Train Loss: 0.0516 | Train accuracy: 0.971356\n",
      "Epoch:  354 | Train Loss: 0.0494 | Test accuracy: 0.969214\n",
      "Epoch:  354 | Train Loss: 0.0494 | Train accuracy: 0.971381\n",
      "Epoch:  355 | Train Loss: 0.0516 | Test accuracy: 0.970260\n",
      "Epoch:  355 | Train Loss: 0.0516 | Train accuracy: 0.971082\n",
      "Epoch:  356 | Train Loss: 0.0493 | Test accuracy: 0.970659\n",
      "Epoch:  356 | Train Loss: 0.0493 | Train accuracy: 0.970995\n",
      "Epoch:  357 | Train Loss: 0.0507 | Test accuracy: 0.970559\n",
      "Epoch:  357 | Train Loss: 0.0507 | Train accuracy: 0.971094\n",
      "Epoch:  358 | Train Loss: 0.0509 | Test accuracy: 0.969413\n",
      "Epoch:  358 | Train Loss: 0.0509 | Train accuracy: 0.971381\n",
      "Epoch:  359 | Train Loss: 0.0497 | Test accuracy: 0.969712\n",
      "Epoch:  359 | Train Loss: 0.0497 | Train accuracy: 0.971256\n",
      "Epoch:  360 | Train Loss: 0.0505 | Test accuracy: 0.969862\n",
      "Epoch:  360 | Train Loss: 0.0505 | Train accuracy: 0.971231\n",
      "Epoch:  361 | Train Loss: 0.0500 | Test accuracy: 0.969862\n",
      "Epoch:  361 | Train Loss: 0.0500 | Train accuracy: 0.971244\n",
      "Epoch:  362 | Train Loss: 0.0534 | Test accuracy: 0.969115\n",
      "Epoch:  362 | Train Loss: 0.0534 | Train accuracy: 0.971431\n",
      "Epoch:  363 | Train Loss: 0.0486 | Test accuracy: 0.969563\n",
      "Epoch:  363 | Train Loss: 0.0486 | Train accuracy: 0.971281\n",
      "Epoch:  364 | Train Loss: 0.0519 | Test accuracy: 0.969513\n",
      "Epoch:  364 | Train Loss: 0.0519 | Train accuracy: 0.971319\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-0b1ccf3d080c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrnn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mretain_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mtest_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrnn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/anaconda3/lib/python3.7/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    164\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m         \"\"\"\n\u001b[0;32m--> 166\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    167\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/anaconda3/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     97\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     98\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "print('======= 2.1(b) Accuracy Report After Chosen Epochs =======')\n",
    "kfold_cv = KFold(n_splits=5, random_state=None, shuffle=True)\n",
    "rnn = RNN_Batch()\n",
    "optimizer = torch.optim.Adam(rnn.parameters(), lr=learning_rate)\n",
    "loss_func = nn.CrossEntropyLoss()\n",
    "n_epoch = 1000 \n",
    "for epoch in range(n_epoch):\n",
    "    accuracies_test = []\n",
    "    accuracies_train = []\n",
    "    for train_index, test_index in kfold_cv.split(np.arange(0, total_samples)):\n",
    "        X_train, X_test, y_train, y_test = embedded_seq_tensor[train_index], embedded_seq_tensor[test_index], target_tensor[train_index], target_tensor[test_index]\n",
    "        optimizer.zero_grad()\n",
    "        output = rnn(X_train)\n",
    "        loss = loss_func(output, y_train) \n",
    "        loss.backward(retain_graph=True)\n",
    "        optimizer.step()\n",
    "        test_output = rnn(X_test)\n",
    "        train_output = rnn(X_train)\n",
    "        pred_test = torch.max(test_output, 1)[1]\n",
    "        pred_train = torch.max(train_output, 1)[1]\n",
    "        accuracies_test.append(accuracy_score(y_test, pred_test))\n",
    "        accuracies_train.append(accuracy_score(y_train, pred_train))\n",
    "    print(\"Epoch: \", epoch, \"| Train Loss: %.4f\" % loss.item(), '| Test accuracy: %f' % np.mean(accuracies_test))\n",
    "    print(\"Epoch: \", epoch, \"| Train Loss: %.4f\" % loss.item(), '| Train accuracy: %f' % np.mean(accuracies_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('======= 2.2(a) Accuracy Report After Five Epochs =======')\n",
    "kfold_cv = KFold(n_splits=5, random_state=None, shuffle=True)\n",
    "lstm = LSTM()\n",
    "optimizer = torch.optim.Adam(lstm.parameters(), lr=learning_rate)\n",
    "loss_func = nn.CrossEntropyLoss()\n",
    "n_epoch = 5 \n",
    "for epoch in range(n_epoch):\n",
    "    accuracies_test = []\n",
    "    accuracies_train = []\n",
    "    for train_index, test_index in kfold_cv.split(np.arange(0, total_samples)):\n",
    "        X_train, X_test, y_train, y_test = embedded_seq_tensor[train_index], embedded_seq_tensor[test_index], target_tensor[train_index], target_tensor[test_index]\n",
    "        optimizer.zero_grad()\n",
    "        output = lstm(X_train)\n",
    "        loss = loss_func(output, y_train) \n",
    "        loss.backward(retain_graph=True)\n",
    "        optimizer.step()\n",
    "        test_output = lstm(X_test)\n",
    "        train_output = lstm(X_train)\n",
    "        pred_test = torch.max(test_output, 1)[1]\n",
    "        pred_train = torch.max(train_output, 1)[1]\n",
    "        accuracies_test.append(accuracy_score(y_test, pred_test))\n",
    "        accuracies_train.append(accuracy_score(y_train, pred_train))\n",
    "    print(\"Epoch: \", epoch, \"| Train Loss: %.4f\" % loss.item(), '| Test accuracy: %f' % np.mean(accuracies_test))\n",
    "    print(\"Epoch: \", epoch, \"| Train Loss: %.4f\" % loss.item(), '| Train accuracy: %f' % np.mean(accuracies_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======= 2.2(b) Accuracy Report After Chosen Epochs =======\n",
      "Epoch:  0 | Train Loss: 1.9653 | Test accuracy: 0.459151\n",
      "Epoch:  0 | Train Loss: 1.9653 | Train accuracy: 0.459761\n",
      "Epoch:  1 | Train Loss: 1.6639 | Test accuracy: 0.511010\n",
      "Epoch:  1 | Train Loss: 1.6639 | Train accuracy: 0.511184\n",
      "Epoch:  2 | Train Loss: 1.5429 | Test accuracy: 0.530537\n",
      "Epoch:  2 | Train Loss: 1.5429 | Train accuracy: 0.529914\n",
      "Epoch:  3 | Train Loss: 1.4851 | Test accuracy: 0.564015\n",
      "Epoch:  3 | Train Loss: 1.4851 | Train accuracy: 0.562718\n",
      "Epoch:  4 | Train Loss: 1.3936 | Test accuracy: 0.598536\n",
      "Epoch:  4 | Train Loss: 1.3936 | Train accuracy: 0.599121\n",
      "Epoch:  5 | Train Loss: 1.3171 | Test accuracy: 0.622796\n",
      "Epoch:  5 | Train Loss: 1.3171 | Train accuracy: 0.622198\n",
      "Epoch:  6 | Train Loss: 1.2565 | Test accuracy: 0.635051\n",
      "Epoch:  6 | Train Loss: 1.2565 | Train accuracy: 0.634951\n",
      "Epoch:  7 | Train Loss: 1.1906 | Test accuracy: 0.652387\n",
      "Epoch:  7 | Train Loss: 1.1906 | Train accuracy: 0.653245\n",
      "Epoch:  8 | Train Loss: 1.1239 | Test accuracy: 0.667531\n",
      "Epoch:  8 | Train Loss: 1.1239 | Train accuracy: 0.668452\n",
      "Epoch:  9 | Train Loss: 1.0548 | Test accuracy: 0.690047\n",
      "Epoch:  9 | Train Loss: 1.0548 | Train accuracy: 0.689163\n",
      "Epoch:  10 | Train Loss: 0.9959 | Test accuracy: 0.710223\n",
      "Epoch:  10 | Train Loss: 0.9959 | Train accuracy: 0.710060\n",
      "Epoch:  11 | Train Loss: 0.9264 | Test accuracy: 0.726462\n",
      "Epoch:  11 | Train Loss: 0.9264 | Train accuracy: 0.727147\n",
      "Epoch:  12 | Train Loss: 0.8774 | Test accuracy: 0.738070\n",
      "Epoch:  12 | Train Loss: 0.8774 | Train accuracy: 0.739165\n",
      "Epoch:  13 | Train Loss: 0.8409 | Test accuracy: 0.754010\n",
      "Epoch:  13 | Train Loss: 0.8409 | Train accuracy: 0.756053\n",
      "Epoch:  14 | Train Loss: 0.8172 | Test accuracy: 0.763276\n",
      "Epoch:  14 | Train Loss: 0.8172 | Train accuracy: 0.763064\n",
      "Epoch:  15 | Train Loss: 0.7512 | Test accuracy: 0.775929\n",
      "Epoch:  15 | Train Loss: 0.7512 | Train accuracy: 0.775207\n",
      "Epoch:  16 | Train Loss: 0.7158 | Test accuracy: 0.783501\n",
      "Epoch:  16 | Train Loss: 0.7158 | Train accuracy: 0.785444\n",
      "Epoch:  17 | Train Loss: 0.6872 | Test accuracy: 0.794211\n",
      "Epoch:  17 | Train Loss: 0.6872 | Train accuracy: 0.793551\n",
      "Epoch:  18 | Train Loss: 0.6531 | Test accuracy: 0.796553\n",
      "Epoch:  18 | Train Loss: 0.6531 | Train accuracy: 0.795145\n",
      "Epoch:  19 | Train Loss: 0.6421 | Test accuracy: 0.804374\n",
      "Epoch:  19 | Train Loss: 0.6421 | Train accuracy: 0.804511\n",
      "Epoch:  20 | Train Loss: 0.6157 | Test accuracy: 0.812245\n",
      "Epoch:  20 | Train Loss: 0.6157 | Train accuracy: 0.810850\n",
      "Epoch:  21 | Train Loss: 0.5938 | Test accuracy: 0.815832\n",
      "Epoch:  21 | Train Loss: 0.5938 | Train accuracy: 0.818347\n",
      "Epoch:  22 | Train Loss: 0.5714 | Test accuracy: 0.822905\n",
      "Epoch:  22 | Train Loss: 0.5714 | Train accuracy: 0.824873\n",
      "Epoch:  23 | Train Loss: 0.5582 | Test accuracy: 0.824649\n",
      "Epoch:  23 | Train Loss: 0.5582 | Train accuracy: 0.824586\n",
      "Epoch:  24 | Train Loss: 0.5373 | Test accuracy: 0.833217\n",
      "Epoch:  24 | Train Loss: 0.5373 | Train accuracy: 0.832943\n",
      "Epoch:  25 | Train Loss: 0.5204 | Test accuracy: 0.838249\n",
      "Epoch:  25 | Train Loss: 0.5204 | Train accuracy: 0.839320\n",
      "Epoch:  26 | Train Loss: 0.4990 | Test accuracy: 0.844625\n",
      "Epoch:  26 | Train Loss: 0.4990 | Train accuracy: 0.844787\n",
      "Epoch:  27 | Train Loss: 0.5780 | Test accuracy: 0.829480\n",
      "Epoch:  27 | Train Loss: 0.5780 | Train accuracy: 0.829431\n",
      "Epoch:  28 | Train Loss: 0.5822 | Test accuracy: 0.828585\n",
      "Epoch:  28 | Train Loss: 0.5822 | Train accuracy: 0.828484\n",
      "Epoch:  29 | Train Loss: 0.5223 | Test accuracy: 0.831972\n",
      "Epoch:  29 | Train Loss: 0.5223 | Train accuracy: 0.830539\n",
      "Epoch:  30 | Train Loss: 0.4958 | Test accuracy: 0.842682\n",
      "Epoch:  30 | Train Loss: 0.4958 | Train accuracy: 0.842632\n",
      "Epoch:  31 | Train Loss: 0.4744 | Test accuracy: 0.852048\n",
      "Epoch:  31 | Train Loss: 0.4744 | Train accuracy: 0.851562\n",
      "Epoch:  32 | Train Loss: 0.4441 | Test accuracy: 0.858773\n",
      "Epoch:  32 | Train Loss: 0.4441 | Train accuracy: 0.858997\n",
      "Epoch:  33 | Train Loss: 0.4261 | Test accuracy: 0.866295\n",
      "Epoch:  33 | Train Loss: 0.4261 | Train accuracy: 0.865236\n",
      "Epoch:  34 | Train Loss: 0.4052 | Test accuracy: 0.869881\n",
      "Epoch:  34 | Train Loss: 0.4052 | Train accuracy: 0.869794\n",
      "Epoch:  35 | Train Loss: 0.3968 | Test accuracy: 0.875162\n",
      "Epoch:  35 | Train Loss: 0.3968 | Train accuracy: 0.875012\n",
      "Epoch:  36 | Train Loss: 0.3838 | Test accuracy: 0.879895\n",
      "Epoch:  36 | Train Loss: 0.3838 | Train accuracy: 0.879571\n",
      "Epoch:  37 | Train Loss: 0.3657 | Test accuracy: 0.884278\n",
      "Epoch:  37 | Train Loss: 0.3657 | Train accuracy: 0.884253\n",
      "Epoch:  38 | Train Loss: 0.3574 | Test accuracy: 0.887317\n",
      "Epoch:  38 | Train Loss: 0.3574 | Train accuracy: 0.887653\n",
      "Epoch:  39 | Train Loss: 0.3393 | Test accuracy: 0.892000\n",
      "Epoch:  39 | Train Loss: 0.3393 | Train accuracy: 0.892336\n",
      "Epoch:  40 | Train Loss: 0.3345 | Test accuracy: 0.895038\n",
      "Epoch:  40 | Train Loss: 0.3345 | Train accuracy: 0.894976\n",
      "Epoch:  41 | Train Loss: 0.3251 | Test accuracy: 0.894441\n",
      "Epoch:  41 | Train Loss: 0.3251 | Train accuracy: 0.894129\n",
      "Epoch:  42 | Train Loss: 0.3151 | Test accuracy: 0.901714\n",
      "Epoch:  42 | Train Loss: 0.3151 | Train accuracy: 0.901041\n",
      "Epoch:  43 | Train Loss: 0.3115 | Test accuracy: 0.904305\n",
      "Epoch:  43 | Train Loss: 0.3115 | Train accuracy: 0.904491\n",
      "Epoch:  44 | Train Loss: 0.2890 | Test accuracy: 0.910531\n",
      "Epoch:  44 | Train Loss: 0.2890 | Train accuracy: 0.910930\n",
      "Epoch:  45 | Train Loss: 0.2726 | Test accuracy: 0.915712\n",
      "Epoch:  45 | Train Loss: 0.2726 | Train accuracy: 0.915500\n",
      "Epoch:  46 | Train Loss: 0.2597 | Test accuracy: 0.919299\n",
      "Epoch:  46 | Train Loss: 0.2597 | Train accuracy: 0.919871\n",
      "Epoch:  47 | Train Loss: 0.2437 | Test accuracy: 0.923283\n",
      "Epoch:  47 | Train Loss: 0.2437 | Train accuracy: 0.923707\n",
      "Epoch:  48 | Train Loss: 0.2681 | Test accuracy: 0.911276\n",
      "Epoch:  48 | Train Loss: 0.2681 | Train accuracy: 0.909261\n",
      "Epoch:  49 | Train Loss: 0.4597 | Test accuracy: 0.777523\n",
      "Epoch:  49 | Train Loss: 0.4597 | Train accuracy: 0.780089\n",
      "Epoch:  50 | Train Loss: 0.5662 | Test accuracy: 0.818621\n",
      "Epoch:  50 | Train Loss: 0.5662 | Train accuracy: 0.818285\n",
      "Epoch:  51 | Train Loss: 0.4720 | Test accuracy: 0.847465\n",
      "Epoch:  51 | Train Loss: 0.4720 | Train accuracy: 0.849469\n",
      "Epoch:  52 | Train Loss: 0.4071 | Test accuracy: 0.868637\n",
      "Epoch:  52 | Train Loss: 0.4071 | Train accuracy: 0.869894\n",
      "Epoch:  53 | Train Loss: 0.3539 | Test accuracy: 0.884926\n",
      "Epoch:  53 | Train Loss: 0.3539 | Train accuracy: 0.885100\n",
      "Epoch:  54 | Train Loss: 0.3131 | Test accuracy: 0.898426\n",
      "Epoch:  54 | Train Loss: 0.3131 | Train accuracy: 0.898090\n",
      "Epoch:  55 | Train Loss: 0.2870 | Test accuracy: 0.907891\n",
      "Epoch:  55 | Train Loss: 0.2870 | Train accuracy: 0.906658\n",
      "Epoch:  56 | Train Loss: 0.2671 | Test accuracy: 0.914765\n",
      "Epoch:  56 | Train Loss: 0.2671 | Train accuracy: 0.915326\n",
      "Epoch:  57 | Train Loss: 0.2475 | Test accuracy: 0.921789\n",
      "Epoch:  57 | Train Loss: 0.2475 | Train accuracy: 0.921254\n",
      "Epoch:  58 | Train Loss: 0.2357 | Test accuracy: 0.927518\n",
      "Epoch:  58 | Train Loss: 0.2357 | Train accuracy: 0.927406\n",
      "Epoch:  59 | Train Loss: 0.2248 | Test accuracy: 0.931603\n",
      "Epoch:  59 | Train Loss: 0.2248 | Train accuracy: 0.932101\n",
      "Epoch:  60 | Train Loss: 0.2134 | Test accuracy: 0.935090\n",
      "Epoch:  60 | Train Loss: 0.2134 | Train accuracy: 0.936074\n",
      "Epoch:  61 | Train Loss: 0.2048 | Test accuracy: 0.938528\n",
      "Epoch:  61 | Train Loss: 0.2048 | Train accuracy: 0.938353\n",
      "Epoch:  62 | Train Loss: 0.2006 | Test accuracy: 0.940471\n",
      "Epoch:  62 | Train Loss: 0.2006 | Train accuracy: 0.940383\n",
      "Epoch:  63 | Train Loss: 0.1854 | Test accuracy: 0.942712\n",
      "Epoch:  63 | Train Loss: 0.1854 | Train accuracy: 0.942612\n",
      "Epoch:  64 | Train Loss: 0.1787 | Test accuracy: 0.944854\n",
      "Epoch:  64 | Train Loss: 0.1787 | Train accuracy: 0.944829\n",
      "Epoch:  65 | Train Loss: 0.1758 | Test accuracy: 0.946947\n",
      "Epoch:  65 | Train Loss: 0.1758 | Train accuracy: 0.947071\n",
      "Epoch:  66 | Train Loss: 0.1680 | Test accuracy: 0.949288\n",
      "Epoch:  66 | Train Loss: 0.1680 | Train accuracy: 0.949275\n",
      "Epoch:  67 | Train Loss: 0.1589 | Test accuracy: 0.951579\n",
      "Epoch:  67 | Train Loss: 0.1589 | Train accuracy: 0.951355\n",
      "Epoch:  68 | Train Loss: 0.1549 | Test accuracy: 0.952725\n",
      "Epoch:  68 | Train Loss: 0.1549 | Train accuracy: 0.952887\n",
      "Epoch:  69 | Train Loss: 0.1514 | Test accuracy: 0.953721\n",
      "Epoch:  69 | Train Loss: 0.1514 | Train accuracy: 0.954082\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  70 | Train Loss: 0.1438 | Test accuracy: 0.954219\n",
      "Epoch:  70 | Train Loss: 0.1438 | Train accuracy: 0.955266\n",
      "Epoch:  71 | Train Loss: 0.1397 | Test accuracy: 0.956063\n",
      "Epoch:  71 | Train Loss: 0.1397 | Train accuracy: 0.956175\n",
      "Epoch:  72 | Train Loss: 0.1381 | Test accuracy: 0.956810\n",
      "Epoch:  72 | Train Loss: 0.1381 | Train accuracy: 0.957644\n",
      "Epoch:  73 | Train Loss: 0.1316 | Test accuracy: 0.957756\n",
      "Epoch:  73 | Train Loss: 0.1316 | Train accuracy: 0.958416\n",
      "Epoch:  74 | Train Loss: 0.1280 | Test accuracy: 0.959301\n",
      "Epoch:  74 | Train Loss: 0.1280 | Train accuracy: 0.959537\n",
      "Epoch:  75 | Train Loss: 0.1193 | Test accuracy: 0.960546\n",
      "Epoch:  75 | Train Loss: 0.1193 | Train accuracy: 0.960658\n",
      "Epoch:  76 | Train Loss: 0.1184 | Test accuracy: 0.961293\n",
      "Epoch:  76 | Train Loss: 0.1184 | Train accuracy: 0.961555\n",
      "Epoch:  77 | Train Loss: 0.1145 | Test accuracy: 0.963136\n",
      "Epoch:  77 | Train Loss: 0.1145 | Train accuracy: 0.962663\n",
      "Epoch:  78 | Train Loss: 0.1112 | Test accuracy: 0.963186\n",
      "Epoch:  78 | Train Loss: 0.1112 | Train accuracy: 0.963585\n",
      "Epoch:  79 | Train Loss: 0.1064 | Test accuracy: 0.964182\n",
      "Epoch:  79 | Train Loss: 0.1064 | Train accuracy: 0.964469\n",
      "Epoch:  80 | Train Loss: 0.1044 | Test accuracy: 0.963934\n",
      "Epoch:  80 | Train Loss: 0.1044 | Train accuracy: 0.964955\n",
      "Epoch:  81 | Train Loss: 0.1017 | Test accuracy: 0.964581\n",
      "Epoch:  81 | Train Loss: 0.1017 | Train accuracy: 0.965665\n",
      "Epoch:  82 | Train Loss: 0.0979 | Test accuracy: 0.965528\n",
      "Epoch:  82 | Train Loss: 0.0979 | Train accuracy: 0.966225\n",
      "Epoch:  83 | Train Loss: 0.0929 | Test accuracy: 0.965926\n",
      "Epoch:  83 | Train Loss: 0.0929 | Train accuracy: 0.966873\n",
      "Epoch:  84 | Train Loss: 0.0943 | Test accuracy: 0.966225\n",
      "Epoch:  84 | Train Loss: 0.0943 | Train accuracy: 0.967184\n",
      "Epoch:  85 | Train Loss: 0.0905 | Test accuracy: 0.966673\n",
      "Epoch:  85 | Train Loss: 0.0905 | Train accuracy: 0.967794\n",
      "Epoch:  86 | Train Loss: 0.0895 | Test accuracy: 0.967471\n",
      "Epoch:  86 | Train Loss: 0.0895 | Train accuracy: 0.968106\n",
      "Epoch:  87 | Train Loss: 0.0868 | Test accuracy: 0.966574\n",
      "Epoch:  87 | Train Loss: 0.0868 | Train accuracy: 0.968878\n",
      "Epoch:  88 | Train Loss: 0.0839 | Test accuracy: 0.967769\n",
      "Epoch:  88 | Train Loss: 0.0839 | Train accuracy: 0.968741\n",
      "Epoch:  89 | Train Loss: 0.0810 | Test accuracy: 0.967670\n",
      "Epoch:  89 | Train Loss: 0.0810 | Train accuracy: 0.969239\n",
      "Epoch:  90 | Train Loss: 0.0791 | Test accuracy: 0.968317\n",
      "Epoch:  90 | Train Loss: 0.0791 | Train accuracy: 0.969326\n",
      "Epoch:  91 | Train Loss: 0.0791 | Test accuracy: 0.968268\n",
      "Epoch:  91 | Train Loss: 0.0791 | Train accuracy: 0.969538\n",
      "Epoch:  92 | Train Loss: 0.0789 | Test accuracy: 0.967670\n",
      "Epoch:  92 | Train Loss: 0.0789 | Train accuracy: 0.969812\n",
      "Epoch:  93 | Train Loss: 0.0762 | Test accuracy: 0.968218\n",
      "Epoch:  93 | Train Loss: 0.0762 | Train accuracy: 0.969787\n",
      "Epoch:  94 | Train Loss: 0.0739 | Test accuracy: 0.968815\n",
      "Epoch:  94 | Train Loss: 0.0739 | Train accuracy: 0.969824\n",
      "Epoch:  95 | Train Loss: 0.0750 | Test accuracy: 0.968566\n",
      "Epoch:  95 | Train Loss: 0.0750 | Train accuracy: 0.970248\n",
      "Epoch:  96 | Train Loss: 0.0722 | Test accuracy: 0.968865\n",
      "Epoch:  96 | Train Loss: 0.0722 | Train accuracy: 0.970272\n",
      "Epoch:  97 | Train Loss: 0.0698 | Test accuracy: 0.969612\n",
      "Epoch:  97 | Train Loss: 0.0698 | Train accuracy: 0.970198\n",
      "Epoch:  98 | Train Loss: 0.0682 | Test accuracy: 0.969612\n",
      "Epoch:  98 | Train Loss: 0.0682 | Train accuracy: 0.970272\n",
      "Epoch:  99 | Train Loss: 0.0685 | Test accuracy: 0.969214\n",
      "Epoch:  99 | Train Loss: 0.0685 | Train accuracy: 0.970522\n",
      "Epoch:  100 | Train Loss: 0.0669 | Test accuracy: 0.969563\n",
      "Epoch:  100 | Train Loss: 0.0669 | Train accuracy: 0.970559\n",
      "Epoch:  101 | Train Loss: 0.0665 | Test accuracy: 0.969065\n",
      "Epoch:  101 | Train Loss: 0.0665 | Train accuracy: 0.970895\n",
      "Epoch:  102 | Train Loss: 0.0646 | Test accuracy: 0.969862\n",
      "Epoch:  102 | Train Loss: 0.0646 | Train accuracy: 0.970833\n",
      "Epoch:  103 | Train Loss: 0.0656 | Test accuracy: 0.969115\n",
      "Epoch:  103 | Train Loss: 0.0656 | Train accuracy: 0.971032\n",
      "Epoch:  104 | Train Loss: 0.0646 | Test accuracy: 0.969513\n",
      "Epoch:  104 | Train Loss: 0.0646 | Train accuracy: 0.970933\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-be7edd6e5af0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0membedded_seq_tensor\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membedded_seq_tensor\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtest_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_tensor\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_tensor\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtest_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlstm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mretain_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    539\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 541\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    542\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-8f418ab45c3b>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m         \u001b[0mr_out\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mh_n\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh_c\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrnn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr_out\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    539\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 541\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    542\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/anaconda3/lib/python3.7/site-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    562\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward_packed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    563\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 564\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    565\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    566\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/anaconda3/lib/python3.7/site-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36mforward_tensor\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    541\u001b[0m         \u001b[0munsorted_indices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    542\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 543\u001b[0;31m         \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_sizes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_batch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msorted_indices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    544\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    545\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermute_hidden\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munsorted_indices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/anaconda3/lib/python3.7/site-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36mforward_impl\u001b[0;34m(self, input, hx, batch_sizes, max_batch_size, sorted_indices)\u001b[0m\n\u001b[1;32m    524\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbatch_sizes\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    525\u001b[0m             result = _VF.lstm(input, hx, self._get_flat_weights(), self.bias, self.num_layers,\n\u001b[0;32m--> 526\u001b[0;31m                               self.dropout, self.training, self.bidirectional, self.batch_first)\n\u001b[0m\u001b[1;32m    527\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    528\u001b[0m             result = _VF.lstm(input, batch_sizes, hx, self._get_flat_weights(), self.bias,\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "print('======= 2.2(b) Accuracy Report After Chosen Epochs =======')\n",
    "kfold_cv = KFold(n_splits=5, random_state=None, shuffle=True)\n",
    "lstm = LSTM()\n",
    "optimizer = torch.optim.Adam(lstm.parameters(), lr=learning_rate)\n",
    "loss_func = nn.CrossEntropyLoss()\n",
    "n_epoch = 1000 \n",
    "for epoch in range(n_epoch):\n",
    "    accuracies_test = []\n",
    "    accuracies_train = []\n",
    "    for train_index, test_index in kfold_cv.split(np.arange(0, total_samples)):\n",
    "        X_train, X_test, y_train, y_test = embedded_seq_tensor[train_index], embedded_seq_tensor[test_index], target_tensor[train_index], target_tensor[test_index]\n",
    "        optimizer.zero_grad()\n",
    "        output = lstm(X_train)\n",
    "        loss = loss_func(output, y_train) \n",
    "        loss.backward(retain_graph=True)\n",
    "        optimizer.step()\n",
    "        test_output = lstm(X_test)\n",
    "        train_output = lstm(X_train)\n",
    "        pred_test = torch.max(test_output, 1)[1]\n",
    "        pred_train = torch.max(train_output, 1)[1]\n",
    "        accuracies_test.append(accuracy_score(y_test, pred_test))\n",
    "        accuracies_train.append(accuracy_score(y_train, pred_train))\n",
    "    print(\"Epoch: \", epoch, \"| Train Loss: %.4f\" % loss.item(), '| Test accuracy: %f' % np.mean(accuracies_test))\n",
    "    print(\"Epoch: \", epoch, \"| Train Loss: %.4f\" % loss.item(), '| Train accuracy: %f' % np.mean(accuracies_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======= 2.3 Accuracy Report =======\n",
      "Epoch:  0 | Train Loss: 1.9788 | Test accuracy: 0.463892\n",
      "Epoch:  0 | Train Loss: 1.9788 | Train accuracy: 0.467744\n",
      "Epoch:  1 | Train Loss: 1.6359 | Test accuracy: 0.512963\n",
      "Epoch:  1 | Train Loss: 1.6359 | Train accuracy: 0.514356\n",
      "Epoch:  2 | Train Loss: 1.5414 | Test accuracy: 0.543095\n",
      "Epoch:  2 | Train Loss: 1.5414 | Train accuracy: 0.542666\n",
      "Epoch:  3 | Train Loss: 1.4581 | Test accuracy: 0.557299\n",
      "Epoch:  3 | Train Loss: 1.4581 | Train accuracy: 0.559540\n",
      "Epoch:  4 | Train Loss: 1.3828 | Test accuracy: 0.606119\n",
      "Epoch:  4 | Train Loss: 1.3828 | Train accuracy: 0.604313\n",
      "Epoch:  5 | Train Loss: 1.3203 | Test accuracy: 0.618276\n",
      "Epoch:  5 | Train Loss: 1.3203 | Train accuracy: 0.618474\n",
      "Epoch:  6 | Train Loss: 1.2545 | Test accuracy: 0.633217\n",
      "Epoch:  6 | Train Loss: 1.2545 | Train accuracy: 0.633082\n",
      "Epoch:  7 | Train Loss: 1.1908 | Test accuracy: 0.646874\n",
      "Epoch:  7 | Train Loss: 1.1908 | Train accuracy: 0.649471\n",
      "Epoch:  8 | Train Loss: 1.1237 | Test accuracy: 0.667405\n",
      "Epoch:  8 | Train Loss: 1.1237 | Train accuracy: 0.669782\n",
      "Epoch:  9 | Train Loss: 1.0526 | Test accuracy: 0.691547\n",
      "Epoch:  9 | Train Loss: 1.0526 | Train accuracy: 0.695053\n",
      "Epoch:  10 | Train Loss: 0.9846 | Test accuracy: 0.717315\n",
      "Epoch:  10 | Train Loss: 0.9846 | Train accuracy: 0.716759\n",
      "Epoch:  11 | Train Loss: 0.9262 | Test accuracy: 0.733497\n",
      "Epoch:  11 | Train Loss: 0.9262 | Train accuracy: 0.734694\n",
      "Epoch:  12 | Train Loss: 0.8927 | Test accuracy: 0.747449\n",
      "Epoch:  12 | Train Loss: 0.8927 | Train accuracy: 0.748417\n",
      "Epoch:  13 | Train Loss: 0.8434 | Test accuracy: 0.757615\n",
      "Epoch:  13 | Train Loss: 0.8434 | Train accuracy: 0.756862\n",
      "Epoch:  14 | Train Loss: 0.7989 | Test accuracy: 0.768469\n",
      "Epoch:  14 | Train Loss: 0.7989 | Train accuracy: 0.769676\n",
      "Epoch:  15 | Train Loss: 0.7519 | Test accuracy: 0.778438\n",
      "Epoch:  15 | Train Loss: 0.7519 | Train accuracy: 0.780785\n",
      "Epoch:  16 | Train Loss: 0.7069 | Test accuracy: 0.791980\n",
      "Epoch:  16 | Train Loss: 0.7069 | Train accuracy: 0.794261\n",
      "Epoch:  17 | Train Loss: 0.6690 | Test accuracy: 0.799656\n",
      "Epoch:  17 | Train Loss: 0.6690 | Train accuracy: 0.802256\n",
      "Epoch:  18 | Train Loss: 0.6357 | Test accuracy: 0.807331\n",
      "Epoch:  18 | Train Loss: 0.6357 | Train accuracy: 0.810214\n",
      "Epoch:  19 | Train Loss: 0.6077 | Test accuracy: 0.815451\n",
      "Epoch:  19 | Train Loss: 0.6077 | Train accuracy: 0.816827\n"
     ]
    }
   ],
   "source": [
    "print('======= 2.3 Accuracy Report =======')\n",
    "skfold_cv = StratifiedKFold(n_splits=5)\n",
    "lstm = LSTM()\n",
    "optimizer = torch.optim.Adam(lstm.parameters(), lr=learning_rate)\n",
    "loss_func = nn.CrossEntropyLoss()\n",
    "n_epoch = 20 \n",
    "for epoch in range(n_epoch):\n",
    "    accuracies_test = []\n",
    "    accuracies_train = []\n",
    "    for Train, Test in skfold_cv.split(embedded_seq_tensor,target_tensor):\n",
    "        optimizer.zero_grad()\n",
    "        output = lstm(embedded_seq_tensor[Train])\n",
    "        loss = loss_func(output, target_tensor[Train]) \n",
    "        loss.backward(retain_graph=True)\n",
    "        optimizer.step()\n",
    "        test_output = lstm(embedded_seq_tensor[Test])\n",
    "        train_output = lstm(embedded_seq_tensor[Train])\n",
    "        pred_test = torch.max(test_output, 1)[1]\n",
    "        pred_train = torch.max(train_output, 1)[1]\n",
    "        accuracy_test = accuracy_score(target_tensor[Test], pred_test)\n",
    "        accuracy_train = accuracy_score(target_tensor[Train], pred_train)\n",
    "        accuracies_test.append(accuracy_test)\n",
    "        accuracies_train.append(accuracy_train)\n",
    "    print(\"Epoch: \", epoch, \"| Train Loss: %.4f\" % loss.item(), '| Test accuracy: %f' % np.mean(accuracies_test))\n",
    "    print(\"Epoch: \", epoch, \"| Train Loss: %.4f\" % loss.item(), '| Train accuracy: %f' % np.mean(accuracies_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
